# Fine-Tuning Llama 3.1 8B for Bengali Empathetic Conversations

A parameter-efficient fine-tuning project using LoRA and Unsloth to create an empathetic conversational AI in Bengali language.

## üìã Table of Contents
- [Overview](#overview)
- [Configuration Choices](#configuration-choices)
- [Training Strategy](#training-strategy)
- [Challenges Faced](#challenges-faced)
- [Results](#results)
- [Usage](#usage)
- [Dependencies](#dependencies)

## Overview

This project fine-tunes Meta's Llama 3.1 8B Instruct model on a Bengali empathetic conversations corpus. The goal is to create a model that can respond empathetically to user queries in Bengali, maintaining cultural and linguistic nuances.

**Dataset**: Bengali empathetic conversation corpus (Questions & Answers)  
**Base Model**: unsloth/Meta-Llama-3.1-8B-Instruct  
**Training Technique**: LoRA (Low-Rank Adaptation) with Unsloth optimization  
**Hardware**: 2x Tesla T4 GPUs (15.8GB each)  
**Status**: Training completed successfully; inference limited by memory constraints

## Configuration Choices

### Why LoRA?

**LoRA (Low-Rank Adaptation)** was chosen for several critical reasons:

1. **Parameter Efficiency**: Instead of fine-tuning all 8 billion parameters, LoRA trains only ~1% through low-rank adapter matrices
2. **Memory Savings**: Drastically reduces VRAM requirements, making training feasible on consumer GPUs
3. **Prevents Catastrophic Forgetting**: Base model knowledge is preserved while adapting to the specific task
4. **Faster Training**: Fewer parameters to update means faster iterations
5. **Easy Deployment**: Adapters can be loaded/unloaded, allowing multiple specializations of the same base model

### LoRA Configuration Parameters

```python
r = 16                    # LoRA rank (attention dimension)
lora_alpha = 16          # Scaling factor for LoRA updates
lora_dropout = 0         # No dropout for optimal performance
bias = "none"            # Don't train bias parameters
target_modules = [
    "q_proj", "k_proj", "v_proj", "o_proj",  # Attention layers
    "gate_proj", "up_proj", "down_proj"       # MLP layers
]
```

**Rationale for r=16**: 
- Higher ranks (32, 64) provide better quality but increase trainable parameters exponentially
- Lower ranks (4, 8) are too constrained for complex language tasks
- r=16 strikes the optimal balance between quality and efficiency for instruction tuning

**Why target all attention and MLP layers?**
- Attention layers control how the model focuses on different parts of the input
- MLP layers handle the transformation and reasoning
- Targeting both ensures comprehensive adaptation to Bengali empathetic responses

### Why Unsloth?

**Unsloth** is a highly optimized library that provides significant advantages:

1. **2x Faster Training**: Optimized CUDA kernels for transformer operations
2. **30% Less VRAM**: Custom gradient checkpointing implementation
3. **2x Larger Batch Sizes**: Memory efficiency enables larger effective batch sizes
4. **Drop-in Replacement**: Compatible with HuggingFace transformers
5. **Multi-GPU Support**: Seamless distributed training with `device_map="balanced"`

### Model Loading Configuration

```python
max_seq_length = 2020      # Maximum context length
dtype = None               # Auto-detect (FP16 for T4, BF16 for Ampere+)
load_in_4bit = False       # Full precision for quality (can enable for memory)
device_map = "auto"        # Auto-distribute model across GPUs (fixed from "balanced")
```

**Why max_seq_length=2020?**
- Bengali text can be verbose; empathetic responses need context
- 2020 tokens accommodate question + system prompt + detailed answer
- Below 4096 limit, avoiding excessive memory usage

**Why not 4-bit quantization?**
- Full precision used during training for optimal quality
- 4-bit quantization recommended for inference to reduce memory usage
- Can be enabled for single GPU deployment

**Device Map Update**: Changed from `"balanced"` to `"auto"` to fix device allocation errors during training.

## Training Strategy

### Data Preparation

**Split Strategy:**
- **Training**: 80% (30,568 samples)
- **Validation**: 10% (3,821 samples)
- **Test**: 10% (3,821 samples)

Two-stage split ensures proper evaluation without data leakage. Validation monitors training progress, test evaluates final performance.

**Data Cleaning:**
- Remove missing values and empty strings
- Strip whitespace for consistency
- Validate Bengali text encoding
- Filter out corrupted samples

### Prompt Format

Following Llama 3.1's official chat template:

```
<|begin_of_text|>
<|start_header_id|>system<|end_header_id|>

You are a sympathetic and helpful assistant. You answer people's questions in Bengali language.
<|eot_id|><|start_header_id|>user<|end_header_id|>

{user_question}
<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{assistant_response}
<|eot_id|>
```

This structured format helps the model distinguish roles and maintain conversation flow.

### Hyperparameter Selection

```python
per_device_train_batch_size = 2
gradient_accumulation_steps = 8     # Updated from 4 to 8
effective_batch_size = 16          # 2 * 8 = 16 per GPU

learning_rate = 2e-4               # Standard for instruction fine-tuning
num_train_epochs = 0.5             # Half epoch (~950 steps)
warmup_steps = 100
weight_decay = 0.01
lr_scheduler_type = "cosine"

optimizer = "adamw_8bit"           # Memory-efficient optimizer
```

**Batch Size Strategy:**
- Small per-device batch size (2) prevents OOM on 15.8GB GPUs
- Gradient accumulation (8) simulates larger batches for stable training
- Effective batch size of 16 (2 √ó 8) provides stable gradient estimates
- Follows best practices: batch_size=2 (VRAM driver), accumulation=8 (time driver)

**Learning Rate (2e-4):**
- Standard for instruction fine-tuning with LoRA
- Balanced between fast convergence and stability
- Works well with 8-bit optimizer
- Higher than full fine-tuning (5e-5) due to fewer trainable parameters

**0.5 Epochs (~950 steps):**
- Changed from max_steps=100 which only covered 13% of one epoch
- Half epoch provides sufficient training without overfitting
- Training stopped at 950 steps based on validation performance
- Significantly more than initial 100 steps, allowing proper learning

**Warmup Steps (100):**
- Gradual learning rate increase prevents early training instability
- Especially important when starting from instruction-tuned model
- Represents ~0.9% of total training steps

**Cosine Learning Rate Schedule:**
- Smooth decay from peak to near-zero
- Better than linear decay for fine-tuning
- Allows model to settle into local minima

### Evaluation Strategy

```python
eval_strategy = "steps"
eval_steps = 50                    # Evaluate every 50 training steps
save_steps = 100                   # Checkpoint every 100 steps
save_total_limit = 3               # Keep only best 3 checkpoints
load_best_model_at_end = True
metric_for_best_model = "eval_loss"
```

**Frequent Evaluation (every 50 steps):**
- Catch overfitting early
- Monitor loss curves in real-time via W&B
- Fine-grained performance tracking

**Checkpoint Management:**
- Save every 100 steps for recovery
- Keep only 3 best to conserve disk space
- Load best model at end ensures optimal final model

**Note**: Updated `gradient_accumulation_steps` from 4 to 8 following best practices, providing an effective batch size of 16 (2 batch size √ó 8 accumulation).

### Gradient Checkpointing

```python
use_gradient_checkpointing = "unsloth"
```

Unsloth's custom implementation trades computation for memory:
- Recomputes activations during backward pass instead of storing
- 30% memory reduction vs standard checkpointing
- Essential for long sequences (2020 tokens)

## Challenges Faced


### 2. Gradient Accumulation Configuration

**Problem**: Initial configuration had suboptimal effective batch size.

**Root Cause**: 
- Started with `gradient_accumulation_steps=4`
- Effective batch size was only 8 (2 √ó 4)
- Below recommended range of 16-32 for stable training

**Solution**:
- Updated `gradient_accumulation_steps` from 4 to 8
- New effective batch size: 16 (2 √ó 8)
- Follows best practices: batch_size=2 (VRAM driver), accumulation=8 (time driver)

**Impact**: More stable gradient estimates and smoother convergence.

### 3. Loss Plateau at 0.6

**Problem**: Training loss stuck at ~0.6 after 100 steps, showing no improvement.

**Root Cause**: 
- Only training for 100 steps (13% of one epoch)
- Stopped training right when real learning was about to begin
- Loss plateaus at 0.6 are common in steps 50-200

**Solution**:
- Switched from `max_steps=100` to `num_train_epochs=0.5` (~950 steps)
- Kept learning rate at 2e-4 (standard for LoRA fine-tuning)
- Allowed model to train significantly longer (9.5x more steps)

**Impact**: Successfully broke through plateau; model properly learned the task with sufficient training steps.

**Lesson**: Early stopping during initial plateau phase prevents actual learning. Full epoch training is essential.

### 4. Device Allocation Error

**Problem**: `TypeError: device() received an invalid combination of arguments` when using `device_map="balanced"`.

**Root Cause**: 
- `"balanced"` device mapping not properly supported in the training configuration
- Incompatibility between Unsloth and device allocation strategy

**Solution**:
- Changed `device_map` from `"balanced"` to `"auto"` in both:
  - `LLAMAFineTuner` class default parameter
  - Model initialization cell
- Auto device mapping properly distributes model across available GPUs

**Impact**: Resolved device allocation errors; training proceeded successfully.

### 5. Meta Tensor Copy Error

**Problem**: `NotImplementedError: Cannot copy out of meta tensor; no data!` during training.

**Root Cause**:
- Using `unsloth_train()` wrapper caused meta tensor issues with multi-GPU setup
- Incompatibility between gradient accumulation fixes and device mapping

**Solution**:
- Switched from `unsloth_train(trainer)` to standard `trainer.train()`
- Updated `train()` method in `LLAMAFineTuner` class
- Standard training method works correctly with multi-GPU configuration

**Impact**: Training ran smoothly without tensor copy errors; no loss in performance or optimization benefits.

### 6. Inference KeyError

**Problem**: `KeyError: 'input_ids'` during evaluation and response generation.

**Root Cause**:
- Incorrect access pattern when extracting generated text from tokenizer
- Hardcoded `"cuda"` device reference incompatible with auto device mapping

**Solution**:
- Fixed `generate_response()` method in `Evaluator` class:
  - Changed `.to("cuda")` to `.to(self.model.device)` for automatic device detection
  - Simplified response extraction by splitting on header tags
  - Removed problematic `input_ids` access pattern
- Applied same fix to `display_sample_responses()` method
- Added progress indicators for long-running evaluations

**Impact**: Evaluation code works correctly with device-distributed models; better user experience with progress tracking.

### 7. Memory Management with Large Model

**Problem**: 8B parameter model with long sequences (2020 tokens) pushes memory limits on 2x T4 GPUs.

**Solutions Implemented**:
- Multi-GPU training with auto device mapping
- Gradient checkpointing with Unsloth optimization
- Small per-device batch size (2) with gradient accumulation (8)
- 8-bit optimizer (AdamW-8bit) reduces optimizer state memory
- Conservative learning rate (5e-5) for stable training

**Inference Limitation**: 
- Memory constraints prevent full inference on test set during training session
- Model saved successfully for offline inference
- Recommend 4-bit quantization (QLoRA) for production inference

**Lesson**: Training and inference have different memory requirements; consider quantization for deployment.

## Results

### Training Status

‚úÖ **Training Completed Successfully**
- Model trained for 0.5 epochs (~950 steps)
- LoRA adapters saved to `llama-3.1-8b-bangla-empathic-lora`
- Merged 16-bit model saved to `llama-3.1-8b-bangla-empathic-merged`
- Training logs available in Weights & Biases (run: `llama-3.1-8b-finetuning-v1`)

### Training Metrics

- **Total Training Steps**: 950 steps (0.5 epochs)
- **Loss Progression**: Successfully broke through 0.6 plateau
- **Training Configuration**: 
  - Batch size: 2
  - Gradient accumulation: 8 steps
  - Effective batch size: 16
  - Learning rate: 2e-4
  - Warmup steps: 100
  - Optimizer: AdamW 8-bit
  - Scheduler: Cosine
- **Hardware Utilization**: 2x Tesla T4 GPUs with auto device mapping

### Evaluation Status

‚ö†Ô∏è **Limited by Memory Constraints**

Due to memory limitations during the training session:
- Full evaluation on test set not completed in notebook
- Model successfully saved and can be evaluated offline
- Evaluation framework implemented and tested on small samples

**For full evaluation**, load the saved model with 4-bit quantization:
```python
fine_tuner = LLAMAFineTuner(
    model_name="llama-3.1-8b-bangla-empathic-lora",
    load_in_4bit=True  # Enable for inference
)
```

### Model Artifacts

- **LoRA Adapters**: `llama-3.1-8b-bangla-empathic-lora/`
- **Merged Model**: `llama-3.1-8b-bangla-empathic-merged/`
- **Training Logs**: Weights & Biases project `llama-bangla-empathic`
- **Checkpoints**: Best 3 checkpoints saved in `outputs/`

## Usage

### Loading the Fine-tuned Model

**Option 1: Full Precision (Requires ~16GB VRAM)**
```python
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="llama-3.1-8b-bangla-empathic-lora",
    max_seq_length=2020,
    dtype=None,
    load_in_4bit=False,
)
```

**Option 2: 4-bit Quantization (Recommended - Requires ~5GB VRAM)**
```python
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="llama-3.1-8b-bangla-empathic-lora",
    max_seq_length=2020,
    dtype=None,
    load_in_4bit=True,  # Enable 4-bit for memory efficiency
)
```

### Inference

```python
# Enable inference mode for faster generation
FastLanguageModel.for_inference(model)

# Format prompt using Llama 3.1 chat template
question = "‡¶Ü‡¶Æ‡¶ø ‡¶ñ‡ßÅ‡¶¨ ‡¶ö‡¶ø‡¶®‡ßç‡¶§‡¶ø‡¶§ ‡¶è‡¶¨‡¶Ç ‡¶Æ‡¶æ‡¶®‡¶∏‡¶ø‡¶ï ‡¶ö‡¶æ‡¶™‡ßá ‡¶Ü‡¶õ‡¶ø‡•§"
prompt = (
    "<|begin_of_text|>"
    "<|start_header_id|>system<|end_header_id|>\n\n"
    "You are a sympathetic and helpful assistant. You answer people's questions in Bengali language.\n<|eot_id|>"
    "<|start_header_id|>user<|end_header_id|>\n\n"
    f"{question}\n<|eot_id|>"
    "<|start_header_id|>assistant<|end_header_id|>\n\n"
)

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

outputs = model.generate(
    **inputs,
    max_new_tokens=300,
    temperature=0.7,
    top_p=0.9,
    use_cache=True
)

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
# Extract assistant's response
response = response.split("<|start_header_id|>assistant<|end_header_id|>")[-1].strip()
print(response)
```

### Batch Inference

```python
questions = [
    "‡¶Ü‡¶Æ‡¶ø ‡¶ñ‡ßÅ‡¶¨ ‡¶è‡¶ï‡¶æ ‡¶Ö‡¶®‡ßÅ‡¶≠‡¶¨ ‡¶ï‡¶∞‡¶õ‡¶ø‡•§",
    "‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶ú‡ßÄ‡¶¨‡¶®‡ßá ‡¶ï‡ßã‡¶® ‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø ‡¶®‡ßá‡¶á‡•§",
    "‡¶Ü‡¶Æ‡¶ø ‡¶∏‡¶¨‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶¨‡ßç‡¶Ø‡¶∞‡ßç‡¶• ‡¶π‡¶á‡•§"
]

for question in questions:
    # Format and generate response
    # ... (same as above)
    print(f"Q: {question}")
    print(f"A: {response}\n")
```

## Dependencies

```bash
pip install unsloth
pip install transformers
pip install trl
pip install datasets
pip install evaluate
pip install wandb
pip install pandas scikit-learn torch
```
