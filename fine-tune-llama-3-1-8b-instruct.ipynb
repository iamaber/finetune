{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "534a6dd0-a490-40e7-9a1d-bcdef943af7a",
    "_uuid": "beba92bc-a60c-4831-9ecf-0a9e3d23db7e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T10:12:20.818406Z",
     "iopub.status.busy": "2025-11-29T10:12:20.817947Z",
     "iopub.status.idle": "2025-11-29T10:12:25.412595Z",
     "shell.execute_reply": "2025-11-29T10:12:25.411663Z",
     "shell.execute_reply.started": "2025-11-29T10:12:20.818369Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.6\n"
     ]
    }
   ],
   "source": [
    "# !pip install unsloth\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "a5d473c7-17e6-421f-b9a1-de701a1f1b5a",
    "_uuid": "07e7faad-c27b-4930-b5ea-b55e47f6d5cf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T06:28:57.641566Z",
     "iopub.status.busy": "2025-11-29T06:28:57.641310Z",
     "iopub.status.idle": "2025-11-29T06:29:38.453143Z",
     "shell.execute_reply": "2025-11-29T06:29:38.452584Z",
     "shell.execute_reply.started": "2025-11-29T06:28:57.641540Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 06:29:04.846229: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764397745.217573      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764397745.342413      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T06:29:38.454092Z",
     "iopub.status.busy": "2025-11-29T06:29:38.453886Z",
     "iopub.status.idle": "2025-11-29T06:29:52.905327Z",
     "shell.execute_reply": "2025-11-29T06:29:52.904402Z",
     "shell.execute_reply.started": "2025-11-29T06:29:38.454074Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maber-islam-dev\u001b[0m (\u001b[33maber-islam-dev-jvai\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20251129_062946-v9tdua94</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic/runs/v9tdua94' target=\"_blank\">llama-3.1-8b-finetuning-v1</a></strong> to <a href='https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic' target=\"_blank\">https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic/runs/v9tdua94' target=\"_blank\">https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic/runs/v9tdua94</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic/runs/v9tdua94?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7d6b2e8dc9d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "my_secret = user_secrets.get_secret(\"wandb_api_key\") \n",
    "wandb.login(key=my_secret)\n",
    "\n",
    "wandb.init(\n",
    "    project=\"llama-bangla-empathic\",\n",
    "    name=\"llama-3.1-8b-finetuning-v2\",\n",
    "    config={\n",
    "        \"model\": \"Llama-3.1-8B-Instruct\",\n",
    "        \"dataset\": \"bangla-empathic\",\n",
    "        \"task\": \"instruction-finetuning\",\n",
    "        \"language\": \"bangla\",\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": 2,\n",
    "        \"gradient_accumulation_steps\": 8,\n",
    "        \"effective_batch_size\": 16,\n",
    "        \"learning_rate\": 5e-5,  # Fixed: Reduced from 2e-4 to 5e-5 for better convergence\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"max_seq_length\": 2020,\n",
    "    },\n",
    "    tags=[\"llama-3.1\", \"bangla\", \"empathic\", \"unsloth\", \"lora\", \"fixed-training\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T06:29:52.906527Z",
     "iopub.status.busy": "2025-11-29T06:29:52.906292Z",
     "iopub.status.idle": "2025-11-29T06:29:52.938226Z",
     "shell.execute_reply": "2025-11-29T06:29:52.937499Z",
     "shell.execute_reply.started": "2025-11-29T06:29:52.906508Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DatasetProcessor:\n",
    "    # Process and format datasets for Llama 3.1 fine-tuning\n",
    "    \n",
    "    def __init__(self, csv_path):\n",
    "        self.csv_path = csv_path\n",
    "        self.df = None\n",
    "        self.train_df = None\n",
    "        self.val_df = None\n",
    "        self.test_df = None\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "    \n",
    "    def load_and_clean_data(self):\n",
    "        # Load and clean the dataset\n",
    "        self.df = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Keep only Questions and Answers columns\n",
    "        self.df = self.df[['Questions', 'Answers']].copy()\n",
    "        \n",
    "        # Remove rows with missing values\n",
    "        self.df = self.df.dropna()\n",
    "        \n",
    "        # Remove rows where text is empty after stripping whitespace\n",
    "        self.df = self.df[(self.df['Questions'].str.strip() != '') & (self.df['Answers'].str.strip() != '')]\n",
    "        \n",
    "        # Strip whitespace\n",
    "        self.df['Questions'] = self.df['Questions'].str.strip()\n",
    "        self.df['Answers'] = self.df['Answers'].str.strip()\n",
    "        \n",
    "        print(f\"Dataset size after cleaning: {len(self.df)}\")\n",
    "        return self.df\n",
    "    \n",
    "    def split_data(self, test_size=0.2, val_size=0.5, random_state=42):\n",
    "        # Split data into train, validation, and test sets\n",
    "        # First split: 80% train, 20% temp (for val + test)\n",
    "        self.train_df, temp_df = train_test_split(\n",
    "            self.df, test_size=test_size, random_state=random_state, shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Second split: Split temp into 50% validation, 50% test (10% each of total)\n",
    "        self.val_df, self.test_df = train_test_split(\n",
    "            temp_df, test_size=val_size, random_state=random_state, shuffle=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Training samples: {len(self.train_df)}\")\n",
    "        print(f\"Validation samples: {len(self.val_df):>6}\")\n",
    "        print(f\"Test samples: {len(self.test_df):>6}\")\n",
    "        \n",
    "        return self.train_df, self.val_df, self.test_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_prompt(question, answer=None):\n",
    "        # Format prompt using Llama 3.1 official format\n",
    "        # Reference: https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/\n",
    "        # Fixed: Added proper newlines before <|eot_id|> tokens\n",
    "        prompt = (\n",
    "            \"<|begin_of_text|>\"\n",
    "            \"<|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "            \"You are a sympathetic and helpful assistant. You answer people's questions in Bengali language.\\n<|eot_id|>\"\n",
    "            \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            f\"{question}\\n<|eot_id|>\"\n",
    "            \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        )\n",
    "        if answer:\n",
    "            prompt += f\"{answer}\\n<|eot_id|>\"\n",
    "        return prompt\n",
    "    \n",
    "    def formatting_prompts_func(self, examples):\n",
    "        # Format dataset examples for training\n",
    "        questions = examples['Questions']\n",
    "        answers = examples['Answers']\n",
    "        texts = [self.format_prompt(q, a) for q, a in zip(questions, answers)]\n",
    "        return {\"text\": texts}\n",
    "    \n",
    "    def create_datasets(self):\n",
    "        # Create HuggingFace datasets from dataframes\n",
    "        self.train_dataset = Dataset.from_pandas(self.train_df[['Questions', 'Answers']].reset_index(drop=True))\n",
    "        self.val_dataset = Dataset.from_pandas(self.val_df[['Questions', 'Answers']].reset_index(drop=True))\n",
    "        self.test_dataset = Dataset.from_pandas(self.test_df[['Questions', 'Answers']].reset_index(drop=True))\n",
    "        \n",
    "        # Apply formatting\n",
    "        self.train_dataset = self.train_dataset.map(self.formatting_prompts_func, batched=True)\n",
    "        self.val_dataset = self.val_dataset.map(self.formatting_prompts_func, batched=True)\n",
    "        self.test_dataset = self.test_dataset.map(self.formatting_prompts_func, batched=True)\n",
    "        \n",
    "        return self.train_dataset, self.val_dataset, self.test_dataset\n",
    "    \n",
    "    def display_sample(self, num_samples=2):\n",
    "        # Display sample formatted data\n",
    "        print(\"Sample formatted training data:\")\n",
    "        print(\"=\"*80)\n",
    "        for i in range(min(num_samples, len(self.train_dataset))):\n",
    "            print(f\"\\nSample {i+1}:\")\n",
    "            print(\"=\"*80)\n",
    "            print(self.train_dataset[i][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LLAMAFineTuner:\n",
    "    # Fine-tune Llama 3.1 model with LoRA\n",
    "    \n",
    "    def __init__(self, model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct\", \n",
    "                 max_seq_length=2000, dtype=None, load_in_4bit=False, device_map=\"balanced\"):\n",
    "        self.model_name = model_name\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.dtype = dtype\n",
    "        self.load_in_4bit = load_in_4bit\n",
    "        self.device_map = device_map\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.trainer = None\n",
    "    \n",
    "    def load_model(self):\n",
    "        # Load the base model and tokenizer\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=self.model_name,\n",
    "            max_seq_length=self.max_seq_length,\n",
    "            dtype=self.dtype,\n",
    "            load_in_4bit=self.load_in_4bit,\n",
    "            device_map=self.device_map\n",
    "        )\n",
    "        print(f\"Model loaded: {self.model_name}\")\n",
    "        return self.model, self.tokenizer\n",
    "    \n",
    "    def apply_lora(self, r=16, lora_alpha=16, lora_dropout=0, bias=\"none\",\n",
    "                   use_gradient_checkpointing=\"unsloth\", random_state=3407):\n",
    "        # Apply LoRA configuration to model\n",
    "        self.model = FastLanguageModel.get_peft_model(\n",
    "            self.model,\n",
    "            r=r, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                          \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout, # Supports any, but = 0 is optimized\n",
    "            bias=bias, # Supports any, but = \"none\" is optimized\n",
    "            # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "            use_gradient_checkpointing=use_gradient_checkpointing, # True or \"unsloth\" for very long context\n",
    "            random_state=random_state,\n",
    "            use_rslora=False, # We support rank stabilized LoRA\n",
    "            loftq_config=None, # And LoftQ\n",
    "        )\n",
    "        print(\"LoRA configuration applied\")\n",
    "        return self.model\n",
    "    \n",
    "    def create_trainer(self, train_dataset, val_dataset, per_device_train_batch_size=2, \n",
    "                       gradient_accumulation_steps=8, num_train_epochs=3, learning_rate=2e-4, \n",
    "                       output_dir=\"outputs\"):\n",
    "        # Create SFT trainer with optimal batch configuration\n",
    "        # Effective Batch Size = per_device_train_batch_size * gradient_accumulation_steps\n",
    "        # Default: 2 * 8 = 16 (recommended for stable training)\n",
    "        \n",
    "        effective_batch_size = per_device_train_batch_size * gradient_accumulation_steps\n",
    "        print(f\"Batch configuration:\")\n",
    "        print(f\"  Per-device batch size: {per_device_train_batch_size}\")\n",
    "        print(f\"  Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "        print(f\"  Effective batch size: {effective_batch_size}\")\n",
    "        \n",
    "        self.trainer = SFTTrainer(\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset, # Use validation set for monitoring\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=self.max_seq_length,\n",
    "            dataset_num_proc=2,\n",
    "            packing=False, # Can make training 5x faster for short sequences\n",
    "            \n",
    "            args=TrainingArguments(\n",
    "                per_device_train_batch_size=per_device_train_batch_size,\n",
    "                per_device_eval_batch_size=2,\n",
    "                gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "                warmup_steps=100, # Increased warmup for better stability\n",
    "                \n",
    "                # Choose one: num_train_epochs OR max_steps\n",
    "                num_train_epochs=num_train_epochs, # For full training - trains through entire dataset\n",
    "                # max_steps=max_steps, # Or use this for quick testing\n",
    "                learning_rate=learning_rate,\n",
    "                fp16=not is_bfloat16_supported(),\n",
    "                bf16=is_bfloat16_supported(),\n",
    "                logging_steps=10,\n",
    "                optim=\"adamw_8bit\",\n",
    "                weight_decay=0.01,\n",
    "                lr_scheduler_type=\"cosine\",\n",
    "                seed=42,\n",
    "                output_dir=output_dir,\n",
    "                \n",
    "                # Evaluation and checkpointing\n",
    "                eval_strategy=\"steps\", # Changed from evaluation_strategy\n",
    "                eval_steps=50, # Evaluate less frequently to speed up training\n",
    "                save_strategy=\"steps\",\n",
    "                save_steps=100, # Save less frequently\n",
    "                save_total_limit=3, # Keep only best 3 checkpoints\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"eval_loss\",\n",
    "                greater_is_better=False,\n",
    "                \n",
    "                # Weights & Biases integration\n",
    "                report_to=\"wandb\", # Enable wandb logging\n",
    "                run_name=\"llama-3.2-8b-finetuning-v2\", # Run name in wandb\n",
    "                logging_first_step=True,\n",
    "                logging_nan_inf_filter=True,\n",
    "                remove_unused_columns=False, # Keep all columns for SFTTrainer\n",
    "            ),\n",
    "        )\n",
    "        print(\"Trainer configured\")\n",
    "        return self.trainer\n",
    "    \n",
    "    def train(self):\n",
    "        # Train the model using standard trainer.train() method\n",
    "        # Note: Using trainer.train() instead of unsloth_train to avoid meta tensor issues\n",
    "        print(\"Starting training...\")\n",
    "        trainer_stats = self.trainer.train()\n",
    "        print(\"Training completed\")\n",
    "        return trainer_stats\n",
    "    \n",
    "    def save_model(self, output_dir=\"llama-3.1-8b-bangla-empathic-lora\"):\n",
    "        # Save the fine-tuned model\n",
    "        self.model.save_pretrained(output_dir)\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        # Optionally save to merged 16bit\n",
    "        self.model.save_pretrained_merged(\"llama-3.1-8b-bangla-empathic-merged\", self.tokenizer, save_method=\"merged_16bit\")\n",
    "        print(f\"Model saved to: {output_dir}\")\n",
    "    \n",
    "    def enable_inference_mode(self):\n",
    "        # Enable inference mode for the model\n",
    "        FastLanguageModel.for_inference(self.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    # Evaluate fine-tuned model\n",
    "    \n",
    "    def __init__(self, fine_tuner, data_processor):\n",
    "        self.model = fine_tuner.model\n",
    "        self.tokenizer = fine_tuner.tokenizer\n",
    "        self.trainer = fine_tuner.trainer\n",
    "        self.test_df = data_processor.test_df\n",
    "        self.test_dataset = data_processor.test_dataset\n",
    "        self.format_prompt = data_processor.format_prompt\n",
    "    \n",
    "    def generate_response(self, question, max_new_tokens=256, temperature=0.7, top_p=0.9):\n",
    "        # Generate a response for a question\n",
    "        prompt = self.format_prompt(question)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            use_cache=True\n",
    "        )\n",
    "        \n",
    "        # Decode and extract only the assistant's response\n",
    "        full_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Extract response after the prompt\n",
    "        response = full_output[len(self.tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)):].strip()\n",
    "        return response\n",
    "    \n",
    "    def evaluate_metrics(self, num_samples=100):\n",
    "        # Evaluate model with BLEU, ROUGE, and Perplexity\n",
    "        import numpy as np\n",
    "        from evaluate import load\n",
    "        \n",
    "        # Load evaluation metrics\n",
    "        bleu_metric = load(\"bleu\")\n",
    "        rouge_metric = load(\"rouge\")\n",
    "        \n",
    "        # Generate predictions on test set\n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        for i in range(min(num_samples, len(self.test_dataset))):\n",
    "            question = self.test_df.iloc[i]['Questions']\n",
    "            reference = self.test_df.iloc[i]['Answers']\n",
    "            prediction = self.generate_response(question)\n",
    "            predictions.append(prediction)\n",
    "            references.append(reference)\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        bleu_results = bleu_metric.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        rouge_results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "        \n",
    "        # Calculate perplexity from evaluation loss\n",
    "        eval_results = self.trainer.evaluate(eval_dataset=self.test_dataset)\n",
    "        perplexity = np.exp(eval_results['eval_loss'])\n",
    "        \n",
    "        results = {\n",
    "            \"perplexity\": perplexity,\n",
    "            \"bleu\": bleu_results['bleu'],\n",
    "            \"rouge1\": rouge_results['rouge1'],\n",
    "            \"rouge2\": rouge_results['rouge2'],\n",
    "            \"rougeL\": rouge_results['rougeL']\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nEvaluation Results:\")\n",
    "        print(f\"Perplexity: {results['perplexity']:.4f}\")\n",
    "        print(f\"BLEU Score: {results['bleu']:.4f}\")\n",
    "        print(f\"ROUGE-1: {results['rouge1']:.4f}\")\n",
    "        print(f\"ROUGE-2: {results['rouge2']:.4f}\")\n",
    "        print(f\"ROUGE-L: {results['rougeL']:.4f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def create_human_eval_samples(self, sample_size=20, output_file=\"human_evaluation_samples.csv\"):\n",
    "        # Create samples for human evaluation\n",
    "        import random\n",
    "        \n",
    "        # Sample random examples for human evaluation\n",
    "        sample_indices = random.sample(range(len(self.test_df)), min(sample_size, len(self.test_df)))\n",
    "        human_eval_data = []\n",
    "        \n",
    "        for idx in sample_indices:\n",
    "            question = self.test_df.iloc[idx]['Questions']\n",
    "            reference = self.test_df.iloc[idx]['Answers']\n",
    "            \n",
    "            # Get model prediction\n",
    "            prediction = self.generate_response(question)\n",
    "            \n",
    "            human_eval_data.append({\n",
    "                \"id\": idx,\n",
    "                \"question\": question,\n",
    "                \"reference_answer\": reference,\n",
    "                \"model_answer\": prediction,\n",
    "                \"empathy_score\": None, # To be filled by human evaluators (1-5)\n",
    "                \"relevance_score\": None, # To be filled by human evaluators (1-5)\n",
    "                \"fluency_score\": None, # To be filled by human evaluators (1-5)\n",
    "                \"notes\": \"\" # Additional comments\n",
    "            })\n",
    "        \n",
    "        # Save to CSV for human evaluation\n",
    "        human_eval_df = pd.DataFrame(human_eval_data)\n",
    "        human_eval_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"\\nCreated {len(human_eval_data)} samples for human evaluation\")\n",
    "        print(f\"Saved to: {output_file}\")\n",
    "        print(\"\\nEvaluation criteria:\")\n",
    "        print(\"1. Empathy Score (1-5): How empathetic and understanding is the response?\")\n",
    "        print(\"2. Relevance Score (1-5): How relevant is the response to the question?\")\n",
    "        print(\"3. Fluency Score (1-5): How fluent and natural is the Bengali language?\")\n",
    "        \n",
    "        return human_eval_df\n",
    "    \n",
    "    def display_sample_responses(self, num_samples=5):\n",
    "        # Display sample responses with streaming\n",
    "        from transformers import TextStreamer\n",
    "        print(f\"Generating {num_samples} sample responses on test prompts\")\n",
    "       \n",
    "        \n",
    "        for i in range(min(num_samples, len(self.test_df))):\n",
    "            question = self.test_df.iloc[i]['Questions']\n",
    "            reference = self.test_df.iloc[i]['Answers']\n",
    "            \n",
    "            print(f\"\\n--- Sample {i+1} ---\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"\\nReference Answer: {reference}\")\n",
    "            print(f\"\\nModel Response:\")\n",
    "            \n",
    "            # Create prompt\n",
    "            prompt = self.format_prompt(question)\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            \n",
    "            # Generate with streaming\n",
    "            text_streamer = TextStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "            _ = self.model.generate(\n",
    "                **inputs,\n",
    "                streamer=text_streamer,\n",
    "                max_new_tokens=300,\n",
    "                temperature=0.5,\n",
    "                top_p=0.9,\n",
    "                use_cache=True\n",
    "            )\n",
    "            \n",
    "            print(\"\\n\" + \"-\"*80)\n",
    "    \n",
    "    def log_all_responses(self, experiment_name=\"llama-3.1-8b-bangla-empathic\"):\n",
    "        # Log all test responses\n",
    "        from datetime import datetime\n",
    "        import uuid\n",
    "        \n",
    "        # Generate experiment ID\n",
    "        experiment_id = f\"{experiment_name}-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "        \n",
    "        # Generate responses for all test samples and log them\n",
    "        generated_responses = []\n",
    "        \n",
    "        print(f\"Generating and logging responses for {len(self.test_df)} test samples...\")\n",
    "        \n",
    "        for idx in range(len(self.test_df)):\n",
    "            question = self.test_df.iloc[idx]['Questions']\n",
    "            reference = self.test_df.iloc[idx]['Answers']\n",
    "            response_text = self.generate_response(question)\n",
    "            \n",
    "            # Create log entry\n",
    "            log_entry = {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"experiment_id\": experiment_id,\n",
    "                \"sample_index\": idx,\n",
    "                \"input_text\": question,\n",
    "                \"reference_text\": reference,\n",
    "                \"response_text\": response_text,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"model_name\": \"llama-3.1-8b-instruct\",\n",
    "                \"temperature\": 0.5,\n",
    "                \"top_p\": 0.9,\n",
    "                \"max_new_tokens\": 300\n",
    "            }\n",
    "            \n",
    "            generated_responses.append(log_entry)\n",
    "            \n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"Processed {idx + 1}/{len(self.test_df)} samples...\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        responses_df = pd.DataFrame(generated_responses)\n",
    "        log_filename = f\"generated_responses_{experiment_id}.csv\"\n",
    "        responses_df.to_csv(log_filename, index=False)\n",
    "        \n",
    "        print(f\"\\nGenerated responses logged successfully!\")\n",
    "        print(f\"Total responses: {len(generated_responses)}\")\n",
    "        print(f\"Saved to: {log_filename}\")\n",
    "        \n",
    "        return responses_df, log_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "eb9412e3-a0ae-4091-bb13-9de72737823a",
    "_uuid": "3b8a32a3-b074-4767-867f-411a4a343ff6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T10:29:07.696485Z",
     "iopub.status.busy": "2025-11-29T10:29:07.695672Z",
     "iopub.status.idle": "2025-11-29T10:29:07.701562Z",
     "shell.execute_reply": "2025-11-29T10:29:07.700927Z",
     "shell.execute_reply.started": "2025-11-29T10:29:07.696458Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 15.8 + 15.8 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(1)}\")\n",
    "    print(\n",
    "        f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} + {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "d0da5b53-af13-4ce5-80b2-78d6cd56251d",
    "_uuid": "eb274f00-f6e9-4d7f-a013-228cbf3b23b1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T06:29:52.998509Z",
     "iopub.status.busy": "2025-11-29T06:29:52.998251Z",
     "iopub.status.idle": "2025-11-29T06:29:53.706944Z",
     "shell.execute_reply": "2025-11-29T06:29:53.706383Z",
     "shell.execute_reply.started": "2025-11-29T06:29:52.998484Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after cleaning: 38210\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶∏‡ßç‡¶§‡ßç‡¶∞‡ßÄ ‡¶è‡¶¨‡¶Ç ‡¶Æ‡¶æ‡¶Ø‡¶º‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶ü‡¶æ‡¶®‡¶ü‡¶æ‡¶® ‡¶Æ‡¶§‡¶¨‡¶ø‡¶∞‡ßã‡¶ß ‡¶ö‡¶≤...</td>\n",
       "      <td>‡¶Ü‡¶™‡¶®‡¶ø ‡¶Ø‡¶æ ‡¶¨‡¶∞‡ßç‡¶£‡¶®‡¶æ ‡¶ï‡¶∞‡¶õ‡ßá‡¶® ‡¶§‡¶æ‡¶ï‡ßá ‡¶Æ‡¶®‡ßã‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®‡ßÄ‡¶∞‡¶æ \"‡¶§‡ßç‡¶∞‡¶ø‡¶≠...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶æ‡¶ö‡ßç‡¶ö‡¶æ ‡¶®‡ßá‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶™‡¶∞‡¶ø‡¶ï‡¶≤‡ßç‡¶™‡¶®‡¶æ ‡¶ï‡¶∞‡¶õ‡¶ø, ‡¶§‡¶æ‡¶á ‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá ‡¶ß...</td>\n",
       "      <td>‡¶π‡¶æ‡¶á‡•§ ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶∂‡¶ø‡¶∂‡ßÅ‡¶∞ (‡¶è‡¶¨‡¶Ç ‡¶®‡¶ø‡¶ú‡ßá‡¶∞) ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ø‡¶æ ‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶Æ‡¶®‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶ó‡ßã‡¶™‡¶® ‡¶Ü‡¶õ‡ßá, ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶Æ‡¶ø ‡¶ú‡¶æ‡¶®‡¶ø ‡¶®‡¶æ ‡¶§‡¶æ‡¶¶‡ßá...</td>\n",
       "      <td>‡¶Æ‡¶®‡ßá ‡¶π‡¶ö‡ßç‡¶õ‡ßá ‡¶ó‡ßã‡¶™‡¶® ‡¶∞‡¶æ‡¶ñ‡¶æ ‡¶è‡¶ñ‡¶® ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá ‡¶Ö‡¶§‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶Ö‡¶ß‡¶ø‡¶ï‡¶æ‡¶∞‡¶∏‡ßÇ‡¶ö‡¶ï...</td>\n",
       "      <td>‡¶π‡ßç‡¶Ø‡¶æ‡¶≤‡ßã‡•§ ‡¶è‡¶ü‡¶æ ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¶‡¶æ‡¶®‡ßç‡¶§ ‡¶Ø‡ßá ‡¶Ü‡¶™‡¶®‡¶ø ‡¶â‡¶™‡¶≤‡¶¨‡ßç‡¶ß‡¶ø ‡¶ï‡¶∞‡¶§‡ßá ‡¶∏‡¶ï‡ßç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡¶ï‡¶Ø‡¶º‡ßá‡¶ï ‡¶¨‡¶õ‡¶∞ ‡¶Ü‡¶ó‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶Æ‡¶æ‡¶•‡¶æ‡¶Ø‡¶º ‡¶Ü‡¶ò‡¶æ‡¶§ ‡¶≤‡ßá‡¶ó‡ßá‡¶õ‡¶ø‡¶≤ ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶Æ‡¶æ...</td>\n",
       "      <td>‡¶Ü‡¶™‡¶®‡¶ø ‡¶¨‡¶≤‡ßá‡¶®‡¶®‡¶ø ‡¶ï‡¶ø ‡¶¨‡¶æ ‡¶ï‡¶§ ‡¶ì‡¶∑‡ßÅ‡¶ß ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶®‡•§ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Questions  \\\n",
       "0  ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶∏‡ßç‡¶§‡ßç‡¶∞‡ßÄ ‡¶è‡¶¨‡¶Ç ‡¶Æ‡¶æ‡¶Ø‡¶º‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶ü‡¶æ‡¶®‡¶ü‡¶æ‡¶® ‡¶Æ‡¶§‡¶¨‡¶ø‡¶∞‡ßã‡¶ß ‡¶ö‡¶≤...   \n",
       "1  ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶æ‡¶ö‡ßç‡¶ö‡¶æ ‡¶®‡ßá‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶™‡¶∞‡¶ø‡¶ï‡¶≤‡ßç‡¶™‡¶®‡¶æ ‡¶ï‡¶∞‡¶õ‡¶ø, ‡¶§‡¶æ‡¶á ‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá ‡¶ß...   \n",
       "2  ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶Æ‡¶®‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶ó‡ßã‡¶™‡¶® ‡¶Ü‡¶õ‡ßá, ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶Æ‡¶ø ‡¶ú‡¶æ‡¶®‡¶ø ‡¶®‡¶æ ‡¶§‡¶æ‡¶¶‡ßá...   \n",
       "3  ‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá ‡¶Ö‡¶§‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶Ö‡¶ß‡¶ø‡¶ï‡¶æ‡¶∞‡¶∏‡ßÇ‡¶ö‡¶ï...   \n",
       "4  ‡¶ï‡¶Ø‡¶º‡ßá‡¶ï ‡¶¨‡¶õ‡¶∞ ‡¶Ü‡¶ó‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶Æ‡¶æ‡¶•‡¶æ‡¶Ø‡¶º ‡¶Ü‡¶ò‡¶æ‡¶§ ‡¶≤‡ßá‡¶ó‡ßá‡¶õ‡¶ø‡¶≤ ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶Æ‡¶æ...   \n",
       "\n",
       "                                             Answers  \n",
       "0  ‡¶Ü‡¶™‡¶®‡¶ø ‡¶Ø‡¶æ ‡¶¨‡¶∞‡ßç‡¶£‡¶®‡¶æ ‡¶ï‡¶∞‡¶õ‡ßá‡¶® ‡¶§‡¶æ‡¶ï‡ßá ‡¶Æ‡¶®‡ßã‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®‡ßÄ‡¶∞‡¶æ \"‡¶§‡ßç‡¶∞‡¶ø‡¶≠...  \n",
       "1  ‡¶π‡¶æ‡¶á‡•§ ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶∂‡¶ø‡¶∂‡ßÅ‡¶∞ (‡¶è‡¶¨‡¶Ç ‡¶®‡¶ø‡¶ú‡ßá‡¶∞) ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ø‡¶æ ‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø...  \n",
       "2  ‡¶Æ‡¶®‡ßá ‡¶π‡¶ö‡ßç‡¶õ‡ßá ‡¶ó‡ßã‡¶™‡¶® ‡¶∞‡¶æ‡¶ñ‡¶æ ‡¶è‡¶ñ‡¶® ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ...  \n",
       "3  ‡¶π‡ßç‡¶Ø‡¶æ‡¶≤‡ßã‡•§ ‡¶è‡¶ü‡¶æ ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¶‡¶æ‡¶®‡ßç‡¶§ ‡¶Ø‡ßá ‡¶Ü‡¶™‡¶®‡¶ø ‡¶â‡¶™‡¶≤‡¶¨‡ßç‡¶ß‡¶ø ‡¶ï‡¶∞‡¶§‡ßá ‡¶∏‡¶ï‡ßç...  \n",
       "4  ‡¶Ü‡¶™‡¶®‡¶ø ‡¶¨‡¶≤‡ßá‡¶®‡¶®‡¶ø ‡¶ï‡¶ø ‡¶¨‡¶æ ‡¶ï‡¶§ ‡¶ì‡¶∑‡ßÅ‡¶ß ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶®‡•§ ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize DatasetProcessor\n",
    "data_processor = DatasetProcessor('/kaggle/input/bengali-empathetic-conversations-corpus/BengaliEmpatheticConversationsCorpus .csv')\n",
    "\n",
    "# Load and clean data\n",
    "df = data_processor.load_and_clean_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "626a8440-adcc-4b5c-9d08-630b7675839b",
    "_uuid": "b00230af-b065-4d04-aa16-104b78ca85d8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T06:29:53.709101Z",
     "iopub.status.busy": "2025-11-29T06:29:53.708897Z",
     "iopub.status.idle": "2025-11-29T06:31:25.124312Z",
     "shell.execute_reply": "2025-11-29T06:31:25.123593Z",
     "shell.execute_reply.started": "2025-11-29T06:29:53.709084Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004ea1393be04b42a7560060f9d15d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd038d1c9d24950b666b383e682197b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768de57438134839981f2d0c7b050d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd561989fb545b296f1a1c6240e7b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f01e00caea4989bc310d1bf93a56fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bcd6f267bd84d239dba99b5a8440168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ec92454dce40af9a60b659f0c8cd57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8231d48a11564b77999500736467e8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98fa82b89da54d048f11ed849560ea1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d871b80d7994f72bcfd611614f2c045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: unsloth/Meta-Llama-3.1-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLAMAFineTuner\n",
    "fine_tuner = LLAMAFineTuner(\n",
    "    model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length=2020, # Choose any! We auto support RoPE Scaling internally!\n",
    "    dtype=None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit=False, # Use 4-bit quantization to reduce memory usage. Can be False.\n",
    "    device_map=\"balanced\" # Enables Multi-GPU Training\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = fine_tuner.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "d1eb6ba2-0f8e-4aeb-9eaa-0162b75ab440",
    "_uuid": "960fd236-32a8-4507-ad68-ee2d6520f0ef",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T06:31:25.125521Z",
     "iopub.status.busy": "2025-11-29T06:31:25.125119Z",
     "iopub.status.idle": "2025-11-29T06:31:25.143281Z",
     "shell.execute_reply": "2025-11-29T06:31:25.142665Z",
     "shell.execute_reply.started": "2025-11-29T06:31:25.125500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 30568\n",
      "Validation samples:   3821\n",
      "Test samples:   3821\n"
     ]
    }
   ],
   "source": [
    "# Train/Validation/Test Splits\n",
    "train_df, val_df, test_df = data_processor.split_data(test_size=0.2, val_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "4ff7045c-791b-426b-b291-87c30bc7de80",
    "_uuid": "1d0ca9d9-6ab8-46a8-9440-1a9e52210a53",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T06:31:25.144235Z",
     "iopub.status.busy": "2025-11-29T06:31:25.143962Z",
     "iopub.status.idle": "2025-11-29T06:31:31.877046Z",
     "shell.execute_reply": "2025-11-29T06:31:31.876377Z",
     "shell.execute_reply.started": "2025-11-29T06:31:25.144211Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a29a936098e4579ac88e4b3d4e76c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30568 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e910a839c97e405f8dcc5f524f13ae90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3e71b1f06e4ef6b969b58913f486bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 4: CREATE HUGGINGFACE DATASETS\n",
    "train_dataset, val_dataset, test_dataset = data_processor.create_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T06:31:31.878117Z",
     "iopub.status.busy": "2025-11-29T06:31:31.877831Z",
     "iopub.status.idle": "2025-11-29T06:31:31.884506Z",
     "shell.execute_reply": "2025-11-29T06:31:31.883723Z",
     "shell.execute_reply.started": "2025-11-29T06:31:31.878100Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample formatted training data:\n",
      "================================================================================\n",
      "\n",
      "Sample 1:\n",
      "================================================================================\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a sympathetic and helpful assistant. You answer people's questions in Bengali language.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶ó‡¶æ‡¶Æ‡ßÄ ‡¶´‡ßá‡¶¨‡ßç‡¶∞‡ßÅ‡¶Ø‡¶º‡¶æ‡¶∞‡¶ø‡¶§‡ßá ‡¶™‡ßç‡¶∞‡¶Æ‡ßã‡¶∂‡¶® ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡¶õ‡¶ø‡•§<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "‡¶Ü‡¶∞‡ßá‡•§ ‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶™‡¶®‡¶ø ‡¶è‡¶ü‡¶æ ‡¶™‡ßá‡¶§‡ßá ‡¶Ü‡¶∂‡¶æ ‡¶ï‡¶∞‡¶ø!<|eot_id|>\n",
      "\n",
      "Sample 2:\n",
      "================================================================================\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a sympathetic and helpful assistant. You answer people's questions in Bengali language.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "‡¶Ü‡¶Æ‡¶ø ‡¶®‡¶ø‡¶ú‡ßá‡¶ï‡ßá ‡¶∞‡¶ï‡ßç‡¶∑‡¶æ ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "‡¶è‡¶ü‡¶æ ‡¶ö‡¶Æ‡ßé‡¶ï‡¶æ‡¶∞, ‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶∂‡¶æ ‡¶ï‡¶∞‡¶ø ‡¶Ü‡¶™‡¶®‡¶ø ‡¶Ü‡¶ò‡¶æ‡¶§ ‡¶™‡¶æ‡¶¨‡ßá‡¶® ‡¶®‡¶æ!<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Display sample data\n",
    "data_processor.display_sample(num_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "5278f42b-88f6-41d8-ab13-6b5de7c292aa",
    "_uuid": "7fc8fa98-3b0f-4126-9601-f1747d95e75e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T06:31:31.885548Z",
     "iopub.status.busy": "2025-11-29T06:31:31.885305Z",
     "iopub.status.idle": "2025-11-29T06:31:38.682859Z",
     "shell.execute_reply": "2025-11-29T06:31:38.682240Z",
     "shell.execute_reply.started": "2025-11-29T06:31:31.885524Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA configuration applied\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA\n",
    "model = fine_tuner.apply_lora(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=2020\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b22387d9-853c-4a3b-8bf3-d8696c98021b",
    "_uuid": "625bdee0-6722-4ae1-9883-913bccb6b12a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T06:31:38.683979Z",
     "iopub.status.busy": "2025-11-29T06:31:38.683692Z",
     "iopub.status.idle": "2025-11-29T06:31:56.235049Z",
     "shell.execute_reply": "2025-11-29T06:31:56.234253Z",
     "shell.execute_reply.started": "2025-11-29T06:31:38.683956Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f2ed1d3a214d16a68b9abd7ffe436f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/30568 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1880cb234e1044549f751f5defc8a2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/3821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer configured\n"
     ]
    }
   ],
   "source": [
    "# Create trainer with optimal batch configuration and full epoch training\n",
    "# Fixed: Changed from max_steps=100 to num_train_epochs=3\n",
    "# - max_steps=100 was only 13% of one epoch (causing loss plateau at 0.6)\n",
    "# - num_train_epochs=3 will train through entire dataset 3 times (~2,250 steps)\n",
    "# - Learning rate reduced to 5e-5 for better convergence past plateaus\n",
    "\n",
    "trainer = fine_tuner.create_trainer(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    per_device_train_batch_size=2,  # Primary driver of VRAM usage\n",
    "    gradient_accumulation_steps=8,  # Primary driver of training time\n",
    "    num_train_epochs=3,  # Train for 3 full epochs instead of stopping at 100 steps\n",
    "    learning_rate=5e-5,  # Reduced from 2e-4 to help break through loss plateaus\n",
    "    output_dir=\"outputs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "14ff21a4-bc89-4e07-9b5d-a64449219000",
    "_uuid": "ae5c4cd4-8a7f-4ede-bc10-27d5add88609",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T06:31:56.236414Z",
     "iopub.status.busy": "2025-11-29T06:31:56.236097Z",
     "iopub.status.idle": "2025-11-29T10:08:46.890125Z",
     "shell.execute_reply": "2025-11-29T10:08:46.889410Z",
     "shell.execute_reply.started": "2025-11-29T06:31:56.236377Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 2\n",
      "   \\\\   /|    Num examples = 30,568 | Num Epochs = 1 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 3:36:18, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.572300</td>\n",
       "      <td>1.555313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.342300</td>\n",
       "      <td>1.171718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.001100</td>\n",
       "      <td>0.888749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.800300</td>\n",
       "      <td>0.726945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.689900</td>\n",
       "      <td>0.696250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.676300</td>\n",
       "      <td>0.655702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.682200</td>\n",
       "      <td>0.642613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.633900</td>\n",
       "      <td>0.637852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.645900</td>\n",
       "      <td>0.634976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.652900</td>\n",
       "      <td>0.630435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>train/loss</td><td>‚ñà‚ñá‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.63044</td></tr><tr><td>eval/runtime</td><td>1234.2125</td></tr><tr><td>eval/samples_per_second</td><td>3.096</td></tr><tr><td>eval/steps_per_second</td><td>1.548</td></tr><tr><td>total_flos</td><td>1.0712537217417216e+16</td></tr><tr><td>train/epoch</td><td>0.02617</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/grad_norm</td><td>0.44142</td></tr><tr><td>train/learning_rate</td><td>0.0002</td></tr><tr><td>train/loss</td><td>0.6529</td></tr><tr><td>train_loss</td><td>0.87083</td></tr><tr><td>train_runtime</td><td>13007.9203</td></tr><tr><td>train_samples_per_second</td><td>0.062</td></tr><tr><td>train_steps_per_second</td><td>0.008</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">llama-3.1-8b-finetuning-v1</strong> at: <a href='https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic/runs/v9tdua94' target=\"_blank\">https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic/runs/v9tdua94</a><br> View project at: <a href='https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic' target=\"_blank\">https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251129_062946-v9tdua94/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer_stats = fine_tuner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "bf6f273a-dd64-4886-98e1-28f1f6056c09",
    "_uuid": "b3f5e14f-4adc-4c03-a051-e098032b11f5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T10:08:46.891464Z",
     "iopub.status.busy": "2025-11-29T10:08:46.891091Z",
     "iopub.status.idle": "2025-11-29T10:08:47.478829Z",
     "shell.execute_reply": "2025-11-29T10:08:47.478221Z",
     "shell.execute_reply.started": "2025-11-29T10:08:46.891439Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: llama-3.1-8b-bangla-empathic-lora\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "fine_tuner.save_model(\"llama-3.1-8b-bangla-empathic-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "0b7c67b3-6a4e-4cf5-b071-82c3541f238b",
    "_uuid": "4a6f28e0-d3bd-4736-90bc-7a574245844b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T10:33:40.116279Z",
     "iopub.status.busy": "2025-11-29T10:33:40.115948Z",
     "iopub.status.idle": "2025-11-29T10:34:53.870797Z",
     "shell.execute_reply": "2025-11-29T10:34:53.869347Z",
     "shell.execute_reply.started": "2025-11-29T10:33:40.116258Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No columns in the dataset match the model's forward method signature: (input_ids, labels, seq_lengths, completion_mask, assistant_masks). The following columns have been ignored: [text, Answers, Questions]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47/1670124344.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Evaluate using: Perplexity, BLEU, ROUGE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Log to wandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_47/2821627983.py\u001b[0m in \u001b[0;36mevaluate_metrics\u001b[0;34m(self, num_samples)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;31m# Calculate perplexity from evaluation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mperplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eval_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4480\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_memory_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4482\u001b[0;31m         \u001b[0meval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_eval_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fsdp_xla_v2_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4484\u001b[0m             \u001b[0meval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtpu_spmd_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mget_eval_dataloader\u001b[0;34m(self, eval_dataset)\u001b[0m\n\u001b[1;32m   1222\u001b[0m         )\n\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m         return self._get_dataloader(\n\u001b[0m\u001b[1;32m   1225\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_get_dataloader\u001b[0;34m(self, dataset, description, batch_size, sampler_fn, is_training, dataloader_key)\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0mdata_collator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_datasets_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_unused_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1096\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0mdata_collator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_collator_with_removed_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_remove_unused_columns\u001b[0;34m(self, dataset, description)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msignature_columns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1022\u001b[0m                 \u001b[0;34mf\"No columns in the dataset match the model's forward method signature: ({', '.join(signature_columns)}). \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m                 \u001b[0;34mf\"The following columns have been ignored: [{', '.join(ignored_columns)}]. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No columns in the dataset match the model's forward method signature: (input_ids, labels, seq_lengths, completion_mask, assistant_masks). The following columns have been ignored: [text, Answers, Questions]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`."
     ]
    }
   ],
   "source": [
    "# Enable inference mode and initialize Evaluator\n",
    "fine_tuner.enable_inference_mode()\n",
    "evaluator = Evaluator(fine_tuner, data_processor)\n",
    "\n",
    "# Evaluate using: Perplexity, BLEU, ROUGE\n",
    "results = evaluator.evaluate_metrics(num_samples=10)\n",
    "\n",
    "# Log to wandb\n",
    "wandb.log(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "df259160-896f-44cd-b081-65ce46bee118",
    "_uuid": "3ab1c13a-f6ee-41f6-a292-fc8d483e5956",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T10:37:00.611468Z",
     "iopub.status.busy": "2025-11-29T10:37:00.610864Z",
     "iopub.status.idle": "2025-11-29T10:39:03.054344Z",
     "shell.execute_reply": "2025-11-29T10:39:03.053690Z",
     "shell.execute_reply.started": "2025-11-29T10:37:00.611443Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 samples for human evaluation\n",
      "Saved to: human_evaluation_samples.csv\n",
      "\n",
      "Evaluation criteria:\n",
      "1. Empathy Score (1-5): How empathetic and understanding is the response?\n",
      "2. Relevance Score (1-5): How relevant is the response to the question?\n",
      "3. Fluency Score (1-5): How fluent and natural is the Bengali language?\n"
     ]
    }
   ],
   "source": [
    "# Evaluate: Human evaluation on empathetic response quality\n",
    "human_eval_df = evaluator.create_human_eval_samples(sample_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "b160cb3a-68c6-4cda-90dd-531d6c958d4b",
    "_uuid": "6398a7d1-4c40-458f-980f-b2a6ce76399e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T10:39:59.653784Z",
     "iopub.status.busy": "2025-11-29T10:39:59.653548Z",
     "iopub.status.idle": "2025-11-29T10:40:38.453792Z",
     "shell.execute_reply": "2025-11-29T10:40:38.453026Z",
     "shell.execute_reply.started": "2025-11-29T10:39:59.653765Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Generating 5 sample responses on test prompts\n",
      "\n",
      "--- Sample 1 ---\n",
      "Question: ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡ßá‡¶∂ ‡¶ï‡¶ø‡¶õ‡ßÅ‡¶¶‡¶ø‡¶® ‡¶ß‡¶∞‡ßá ‡¶¨‡¶ø‡¶∑‡¶®‡ßç‡¶®‡¶§‡¶æ‡¶Ø‡¶º ‡¶≠‡ßÅ‡¶ó‡¶õ‡¶ø‡•§ ‡¶Ü‡¶Æ‡¶ø ‡¶è‡¶ü‡¶æ‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡¶õ‡¶ø, ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡ßá‡¶Æ‡¶ø‡¶ï ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶∂‡¶ø‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá. ‡¶§‡¶ø‡¶®‡¶ø‡¶á ‡¶è‡¶ï‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø ‡¶Ø‡¶ø‡¶®‡¶ø ‡¶Ü‡¶Æ‡¶ø ‡¶Ø‡ßá ‡¶ï‡ßã‡¶®‡¶ì ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡ßá ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ø‡¶•‡ßá‡¶∑‡ßç‡¶ü ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡¶æ‡¶∏ ‡¶ï‡¶∞‡¶ø, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶§‡¶ø‡¶®‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶ø‡¶∑‡¶£‡ßç‡¶®‡¶§‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶∞‡¶£‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï ‡¶õ‡¶ø‡¶®‡ßç‡¶® ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶∏‡¶ø‡¶¶‡ßç‡¶ß‡¶æ‡¶®‡ßç‡¶§ ‡¶®‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤‡ßá‡¶®‡•§ ‡¶Ü‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶•‡ßá‡¶∞‡¶æ‡¶™‡¶ø‡¶∏‡ßç‡¶ü ‡¶¨‡¶æ ‡¶Ö‡¶®‡ßç‡¶Ø ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶¶‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶∏‡¶æ‡¶Æ‡¶∞‡ßç‡¶•‡ßç‡¶Ø ‡¶®‡ßá‡¶á, ‡¶§‡¶æ‡¶á ‡¶§‡¶ø‡¶®‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶•‡¶æ‡¶ï‡¶æ‡¶∞ ‡¶ï‡ßã‡¶® ‡¶Ü‡¶∂‡¶æ ‡¶¶‡ßá‡¶ñ‡ßá‡¶®‡¶®‡¶ø‡•§ ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶á‡¶®‡ßç‡¶∏‡ßç‡¶Ø‡ßÅ‡¶∞‡ßá‡¶®‡ßç‡¶∏‡ßá $‡ß´‡ß¶‡ß¶‡ß¶ ‡¶°‡¶ø‡¶°‡¶æ‡¶ï‡ßç‡¶ü‡¶ø‡¶¨‡¶≤ ‡¶Ü‡¶õ‡ßá, ‡¶§‡¶æ‡¶á ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶ø‡¶∞‡¶ï‡ßç‡¶§‡•§ ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶Ü‡¶§‡ßç‡¶Æ‡¶π‡¶§‡ßç‡¶Ø‡¶æ‡¶∞ ‡¶ö‡¶ø‡¶®‡ßç‡¶§‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡ßá ‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø ‡¶õ‡ßá‡¶°‡¶º‡ßá ‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶∏‡¶¨‡¶ï‡¶ø‡¶õ‡ßÅ‡¶á ‡¶õ‡¶ø‡¶≤‡•§ ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶è‡¶ï‡¶ü‡¶æ‡¶ì ‡¶®‡¶æ‡¶á. ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶™‡¶∞‡¶ø‡¶¨‡¶æ‡¶∞ ‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá ‡¶™‡¶æ‡¶§‡ßç‡¶§‡¶æ ‡¶¶‡ßá‡¶Ø‡¶º ‡¶®‡¶æ; ‡¶ï‡¶Ø‡¶º‡ßá‡¶ï ‡¶¨‡¶õ‡¶∞ ‡¶Ü‡¶ó‡ßá ‡¶Ø‡¶ñ‡¶® ‡¶Ü‡¶Æ‡¶ø ‡ßß‡ßÆ ‡¶¨‡¶õ‡¶∞ ‡¶¨‡¶Ø‡¶º‡¶∏‡ßÄ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤‡¶æ‡¶Æ ‡¶§‡¶ñ‡¶® ‡¶§‡¶æ‡¶∞‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤‡•§ ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶®‡ßç‡¶ß‡ßÅ ‡¶õ‡¶ø‡¶≤, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶§‡¶æ‡¶∞‡¶æ ‡¶∏‡¶¨‡¶æ‡¶á ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ú‡ßÄ‡¶¨‡¶® ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶ö‡¶≤‡ßá ‡¶ó‡ßá‡¶õ‡ßá ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡ßã‡¶® ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶®‡ßá‡¶á‡•§ ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶æ‡¶ï‡ßç‡¶§‡¶® ‡¶™‡ßç‡¶∞‡ßá‡¶Æ‡¶ø‡¶ï ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡¶õ‡ßá ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶™‡¶∞‡¶ø‡¶∏‡ßç‡¶•‡¶ø‡¶§‡¶ø ‡¶Ü‡¶∞‡¶ì ‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™ ‡¶ï‡¶∞‡¶õ‡ßá ‡¶ï‡¶æ‡¶∞‡¶£ ‡¶Ø‡¶§‡¶¨‡¶æ‡¶∞‡¶á ‡¶Ü‡¶Æ‡¶ø ‡¶§‡¶æ‡¶ï‡ßá ‡¶¶‡ßá‡¶ñ‡¶ø ‡¶¨‡¶æ ‡¶∂‡ßÅ‡¶®‡¶ø, ‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶æ‡¶®‡ßç‡¶®‡¶æ‡¶Ø‡¶º ‡¶≠‡ßá‡¶ô‡ßá ‡¶™‡¶°‡¶º‡ßá‡•§ ‡¶Ü‡¶Æ‡¶ø ‡¶ú‡¶æ‡¶®‡¶§‡ßá ‡¶™‡ßá‡¶∞‡ßá‡¶õ‡¶ø ‡¶Ø‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶π‡¶§‡¶æ‡¶∂‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶∞‡¶£‡ßá ‡¶è‡¶ñ‡¶® ‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶ñ‡¶®‡¶á ‡¶™‡¶¶‡ßã‡¶®‡ßç‡¶®‡¶§‡¶ø ‡¶™‡¶æ‡¶¨ ‡¶®‡¶æ‡•§\n",
      "\n",
      "Reference Answer: ‡¶Æ‡¶®‡ßá ‡¶π‡¶ö‡ßç‡¶õ‡ßá ‡¶Ü‡¶™‡¶®‡¶ø ‡¶¨‡ßá‡¶∂ ‡¶ï‡¶ø‡¶õ‡ßÅ‡¶¶‡¶ø‡¶® ‡¶ß‡¶∞‡ßá ‡¶è‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶≤‡¶°‡¶º‡¶æ‡¶á ‡¶ï‡¶∞‡¶õ‡ßá‡¶®, ‡¶è‡¶ï‡¶á ‡¶∞‡¶ï‡¶Æ ‡¶Ö‡¶®‡ßá‡¶ï‡¶ó‡ßÅ‡¶≤‡¶ø ‡¶Æ‡ßã‡¶ï‡¶æ‡¶¨‡¶ø‡¶≤‡¶æ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ï‡ßå‡¶∂‡¶≤ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶Ø‡¶æ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá ‡¶è‡¶¨‡¶Ç ‡¶∏‡¶§‡ßç‡¶Ø ‡¶¨‡¶≤‡ßá ‡¶Æ‡¶®‡ßá ‡¶π‡¶ö‡ßç‡¶õ‡ßá ‡¶®‡¶æ‡•§ ‡¶™‡ßç‡¶∞‡¶æ‡¶Ø‡¶º‡¶∂‡¶á, ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶Æ‡ßã‡¶ï‡¶æ‡¶¨‡¶ø‡¶≤‡¶æ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ï‡ßå‡¶∂‡¶≤‡¶ó‡ßÅ‡¶≤‡¶ø‡¶§‡ßá ‡¶´‡¶ø‡¶∞‡ßá ‡¶Ø‡¶æ‡¶á ‡¶Ø‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶¨‡¶ø‡¶∑‡¶£‡ßç‡¶®‡¶§‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ‡¶Ø‡¶º (‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡ßá‡¶∏, ‡¶∏‡¶æ‡¶Æ‡¶æ‡¶ú‡¶ø‡¶ï ‡¶â‡¶¶‡ßç‡¶¨‡ßá‡¶ó, ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø) ‡¶∏‡¶π‡¶æ‡¶Ø‡¶º‡¶ï ‡¶π‡¶¨‡ßá, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶∏‡¶§‡ßç‡¶Ø ‡¶π‡¶≤ ‡¶¨‡¶ø‡¶∑‡¶£‡ßç‡¶®‡¶§‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ü‡¶ø‡¶∞ ‡¶®‡¶ø‡¶ú‡¶∏‡ßç‡¶¨ ‡¶õ‡ßã‡¶ü ‡¶ü‡ßÅ‡¶≤ ‡¶ï‡¶ø‡¶ü ‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶®‡ß∑    ‡¶¨‡¶ø‡¶∑‡¶£‡ßç‡¶®‡¶§‡¶æ ‡¶∏‡¶∞‡ßç‡¶¨‡¶¨‡ßç‡¶Ø‡¶æ‡¶™‡ßÄ, ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶™‡¶®‡¶ø ‡¶Ø‡¶æ ‡¶≤‡¶ø‡¶ñ‡ßá‡¶õ‡ßá‡¶® ‡¶§‡¶æ‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø ‡¶ï‡¶∞‡ßá, ‡¶≤‡¶ï‡ßç‡¶∑‡¶£‡¶ó‡ßÅ‡¶≤‡¶ø ‡¶á‡¶§‡¶ø‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá‡¶á ‡¶ï‡¶æ‡¶ú ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶ó‡¶§ ‡¶ú‡ßÄ‡¶¨‡¶®‡¶ï‡ßá ‡¶Ö‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø ‡¶â‡¶™‡¶æ‡¶Ø‡¶º‡ßá ‡¶™‡ßç‡¶∞‡¶≠‡¶æ‡¶¨‡¶ø‡¶§ ‡¶ï‡¶∞‡¶õ‡ßá‡•§  ‡¶Ö‡¶ó‡ßç‡¶∞‡¶ó‡¶§‡¶ø‡¶∞ ‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶§‡ßç‡¶§‡¶Æ ‡¶∞‡ßÅ‡¶ü ‡¶π‡¶¨‡ßá ‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶∏‡ßá‡¶≤‡¶ø‡¶Ç ‡¶ñ‡ßã‡¶Å‡¶ú‡¶æ, ‡¶∏‡ßç‡¶¨‡ßÄ‡¶ï‡¶æ‡¶∞‡ßç‡¶Ø, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶Ø‡¶¶‡¶ø ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶¨‡ßÄ‡¶Æ‡¶æ ‡¶ï‡¶∞‡ßç‡¶§‡¶®‡¶Ø‡ßã‡¶ó‡ßç‡¶Ø ‡¶ñ‡ßÅ‡¶¨ ‡¶¨‡ßá‡¶∂‡¶ø ‡¶π‡¶Ø‡¶º, ‡¶§‡¶¨‡ßá ‡¶è‡¶ü‡¶ø ‡¶ñ‡ßÅ‡¶¨ ‡¶¨‡ßç‡¶Ø‡¶Ø‡¶º‡¶¨‡¶π‡ßÅ‡¶≤ ‡¶π‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§ ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶®‡¶ø‡¶Ø‡¶º‡ßã‡¶ó‡¶ï‡¶∞‡ßç‡¶§‡¶æ‡¶∞ ‡¶¶‡ßç‡¶¨‡¶æ‡¶∞‡¶æ ‡¶™‡ßç‡¶∞‡¶∏‡ßç‡¶§‡¶æ‡¶¨‡¶ø‡¶§ ‡¶π‡¶≤‡ßá ‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßç‡¶Æ‡¶ö‡¶æ‡¶∞‡ßÄ ‡¶∏‡¶π‡¶æ‡¶Ø‡¶º‡¶§‡¶æ ‡¶™‡ßç‡¶∞‡ßã‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ (EAP) ‡¶è‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶Ø‡ßã‡¶ó‡¶æ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶∏‡ßÅ‡¶™‡¶æ‡¶∞‡¶ø‡¶∂ ‡¶ï‡¶∞‡¶¨‡•§ ‡¶Ø‡¶¶‡¶ø ‡¶®‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶∏‡ßá‡¶≤‡¶ø‡¶Ç ‡¶¨‡¶ø‡¶ï‡¶≤‡ßç‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡¶Æ‡¶ø‡¶â‡¶®‡¶ø‡¶ü‡¶ø ‡¶∏‡ßá‡¶®‡ßç‡¶ü‡¶æ‡¶∞‡ßá ‡¶ñ‡ßã‡¶Å‡¶ú ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®, ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶°‡¶æ‡¶ï‡ßç‡¶§‡¶æ‡¶∞ ‡¶Ü‡¶™‡¶®‡¶æ‡¶ï‡ßá ‡¶∞‡ßá‡¶´‡¶æ‡¶∞‡ßá‡¶≤ ‡¶¨‡¶æ ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡ßÄ‡¶Ø‡¶º ‡¶ß‡¶∞‡ßç‡¶Æ‡ßÄ‡¶Ø‡¶º ‡¶∏‡¶Ç‡¶∏‡ßç‡¶•‡¶æ ‡¶¶‡ßá‡¶¨‡ßá‡¶® ‡¶ï‡¶ø‡¶®‡¶æ ‡¶§‡¶æ ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ ‡¶ï‡¶∞‡ßá ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®‡ß∑ , ‡¶∏‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¶‡¶æ‡¶®‡ßç‡¶§ ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡ßç‡¶ï‡¶¨‡ßÅ‡¶ï ‡¶∞‡¶Ø‡¶º‡ßá‡¶õ‡ßá ‡¶Ø‡ßá‡¶ó‡ßÅ‡¶≤‡¶ø ‡¶Ü‡¶™‡¶®‡¶ø ‡¶®‡¶ø‡¶ú‡ßá ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶æ ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶•‡ßá‡¶∞‡¶æ‡¶™‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶® (‡¶Ø‡¶§‡¶ï‡ßç‡¶∑‡¶£ ‡¶®‡¶æ ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶•‡ßá‡¶∞‡¶æ‡¶™‡¶ø‡¶∏‡ßç‡¶ü ‡¶ï‡¶ó‡¶®‡¶ø‡¶ü‡¶ø‡¶≠ ‡¶¨‡¶ø‡¶π‡ßá‡¶≠‡¶ø‡¶ì‡¶∞‡¶æ‡¶≤ ‡¶•‡ßá‡¶∞‡¶æ‡¶™‡¶ø ‡¶¨‡¶æ CBT-‡¶§‡ßá ‡¶™‡ßç‡¶∞‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶ø‡¶§ ‡¶•‡¶æ‡¶ï‡ßá), ‡¶Ø‡ßá‡¶Æ‡¶® \"‡¶Æ‡¶æ‡¶á‡¶®‡ßç‡¶° ‡¶ì‡¶≠‡¶æ‡¶∞ ‡¶Æ‡ßÅ‡¶°\"‡•§\n",
      "\n",
      "Model Response:\n",
      "‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶∂‡¶æ ‡¶ï‡¶∞‡¶ø ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶∏‡¶¨‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶≠‡¶æ‡¶≤ ‡¶π‡¶¨‡ßá. ‡¶Ü‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶¨‡¶æ‡¶°‡¶º‡¶ø‡¶∞ ‡¶Æ‡¶æ‡¶≤‡¶ø‡¶ï ‡¶Ø‡¶æ‡¶∞ ‡¶™‡ßÅ‡¶∞‡ßã ‡¶™‡¶∞‡¶ø‡¶¨‡¶æ‡¶∞ ‡¶Ü‡¶õ‡ßá ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶ö‡ßç‡¶ö‡¶æ‡¶¶‡ßá‡¶∞ ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶Ü‡¶Æ‡¶ø ‡¶ñ‡ßÅ‡¶¨ ‡¶ï‡¶Æ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶™‡¶æ‡¶á‡•§ ‡¶Ü‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶™‡ßÅ‡¶≤‡¶ø‡¶∂ ‡¶Ö‡¶´‡¶ø‡¶∏‡¶æ‡¶∞ ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶∏‡ßç‡¶§‡ßç‡¶∞‡ßÄ ‡¶è‡¶ï‡¶ú‡¶® ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑ ‡¶Ø‡¶æ‡¶∞ ‡¶¶‡ßÅ‡¶ü‡¶ø ‡¶∏‡¶®‡ßç‡¶§‡¶æ‡¶® ‡¶Ü‡¶õ‡ßá ‡¶è‡¶¨ÔøΩ\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Sample 2 ---\n",
      "Question: ‡¶è‡¶ï‡¶ü‡¶ø ‡¶è‡¶Æ‡¶Ü‡¶∞‡¶Ü‡¶á ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶®, ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø?\n",
      "\n",
      "Reference Answer: ‡¶®‡¶æ, ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶≤‡¶§‡ßá ‡¶ö‡¶æ‡¶á‡¶õ‡¶ø‡¶≤‡¶æ‡¶Æ ‡¶Ø‡ßá ‡¶Ü‡¶™‡¶®‡¶ø ‡¶Ö‡¶∞‡ßç‡¶•‡¶æ‡¶Ø‡¶º‡¶® ‡¶™‡¶æ‡¶¨‡ßá‡¶®‡•§ ‡¶Æ‡¶ú‡¶æ‡¶¶‡¶æ‡¶∞!!\n",
      "\n",
      "Model Response:\n",
      "‡¶Ü‡¶Æ‡¶ø ‡¶è‡¶ü‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Æ‡¶®‡ßá ‡¶ï‡¶∞‡¶ø\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Sample 3 ---\n",
      "Question: ‡¶¨‡¶ø‡¶ó‡¶§ ‡¶¨‡¶õ‡¶∞‡ßá, ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶ø‡¶ü‡¶ï‡¶Ø‡¶º‡ßá‡¶®‡ßá‡¶∞ ‡¶Æ‡¶§‡ßã ‡¶ï‡ßç‡¶∞‡¶ø‡¶™‡ßç‡¶ü‡ßã‡¶ï‡¶æ‡¶∞‡ßá‡¶®‡ßç‡¶∏‡¶ø‡¶§‡ßá ‡¶¨‡ßá‡¶∂ ‡¶ï‡¶ø‡¶õ‡ßÅ‡¶ü‡¶æ ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø... ‡¶Ø‡¶¶‡¶ø‡¶ì ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø ‡¶Ü‡¶ï‡¶æ‡¶∂‡¶ö‡ßÅ‡¶Æ‡ßç‡¶¨‡ßÄ ‡¶õ‡¶ø‡¶≤, ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶ï‡ßã‡¶®‡ßã ‡¶≤‡¶æ‡¶≠ ‡¶π‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶Ü‡¶ó‡ßá‡¶á ‡¶§‡¶æ ‡¶ï‡¶Æ‡ßá ‡¶ó‡ßá‡¶õ‡ßá‡•§ ‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™ ‡¶Ö‡¶®‡ßÅ‡¶≠‡ßÇ‡¶§‡¶ø!\n",
      "\n",
      "Reference Answer: ‡¶Ü‡¶Æ‡¶ø ‡¶è‡¶ü‡¶æ ‡¶ò‡¶ü‡¶§‡ßá ‡¶¶‡ßá‡¶ñ‡ßá‡¶õ‡¶ø. ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶ø‡¶ü‡¶ï‡¶Ø‡¶º‡ßá‡¶® ‡¶ü‡ßç‡¶∞‡ßá‡¶°‡¶ø‡¶Ç ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶ó‡¶¨‡ßá‡¶∑‡¶£‡¶æ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ï‡¶•‡¶æ ‡¶≠‡¶æ‡¶¨‡¶õ‡¶ø‡¶≤‡¶æ‡¶Æ‡•§\n",
      "\n",
      "Model Response:\n",
      "‡¶Ü‡¶™‡¶®‡¶ø ‡¶Ø‡ßá ‡¶ï‡ßç‡¶∞‡¶ø‡¶™‡ßç‡¶ü‡ßã‡¶ï‡¶æ‡¶∞‡ßá‡¶®‡ßç‡¶∏‡¶ø‡¶§‡ßá ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶® ‡¶§‡¶æ ‡¶ñ‡ßÅ‡¶¨ ‡¶≠‡¶æ‡¶≤ ‡¶õ‡¶ø‡¶≤! ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶Ü‡¶ó‡¶æ‡¶Æ‡ßÄ ‡¶¨‡¶õ‡¶∞‡ßá ‡¶è‡¶ü‡¶ø ‡¶Ü‡¶¨‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡¶§‡ßá ‡¶ö‡¶æ‡¶≤‡¶æ‡¶¨‡ßá‡¶®?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Sample 4 ---\n",
      "Question: ‡¶è‡¶á ‡¶ó‡ßç‡¶∞‡ßÄ‡¶∑‡ßç‡¶Æ ‡¶Ö‡¶®‡ßá‡¶ï ‡¶¶‡ßÄ‡¶∞‡ßç‡¶ò ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá.\n",
      "\n",
      "Reference Answer: ‡¶è‡¶ü‡¶æ ‡¶ï‡ßá‡¶®\n",
      "\n",
      "Model Response:\n",
      "‡¶è‡¶ü‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ó‡ßç‡¶∞‡ßÄ‡¶∑‡ßç‡¶Æ ‡¶π‡¶§‡ßá ‡¶¶‡ßá‡¶ñ‡¶æ‡¶Ø‡¶º ‡¶Ø‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶Æ‡¶®‡ßá ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Sample 5 ---\n",
      "Question: ‡¶π‡ßç‡¶Ø‡¶æ‡¶Å ‡¶§‡¶ø‡¶®‡¶ø ‡¶∏‡¶§‡ßç‡¶Ø‡¶ø‡¶á ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑.\n",
      "\n",
      "Reference Answer: ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶≠‡¶æ‡¶¨‡ßÅ‡¶®, ‡¶Ü‡¶ú‡¶ï‡ßá ‡¶ï‡¶æ‡¶≤‡¶ï‡ßá‡¶∞ ‡¶∏‡ßç‡¶Æ‡ßÉ‡¶§‡¶ø ‡¶π‡¶Ø‡¶º‡ßá ‡¶•‡¶æ‡¶ï‡¶¨‡ßá ‡¶Ø‡¶ñ‡¶® ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶´‡¶ø‡¶∞‡ßá ‡¶§‡¶æ‡¶ï‡¶æ‡¶á\n",
      "\n",
      "Model Response:\n",
      "‡¶ï‡ßá‡¶â ‡¶§‡¶æ‡¶ï‡ßá ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶™‡¶æ‡¶Ø‡¶º? ‡¶ï‡¶ø ‡¶§‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶õ‡ßá?\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample model responses on test prompts\n",
    "evaluator.display_sample_responses(num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1fa3e950-b4ff-460b-8000-f92145bf0241",
    "_uuid": "a5ca8ca0-87e8-43ac-b2b4-6634f7569ca2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T10:41:27.284423Z",
     "iopub.status.busy": "2025-11-29T10:41:27.283703Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating and logging responses for 3821 test samples...\n",
      "Processed 10/3821 samples...\n",
      "Processed 20/3821 samples...\n",
      "Processed 30/3821 samples...\n",
      "Processed 40/3821 samples...\n",
      "Processed 50/3821 samples...\n",
      "Processed 60/3821 samples...\n",
      "Processed 70/3821 samples...\n"
     ]
    }
   ],
   "source": [
    "# Store logs for GeneratedResponses: id, experiment_id, input_text, response_text, timestamp\n",
    "responses_df, log_filename = evaluator.log_all_responses()\n",
    "\n",
    "# Upload to wandb as artifact\n",
    "artifact = wandb.Artifact(\n",
    "    name=\"generated-responses\",\n",
    "    type=\"predictions\",\n",
    "    description=\"Generated responses from fine-tuned model on test set\"\n",
    ")\n",
    "artifact.add_file(log_filename)\n",
    "wandb.log_artifact(artifact)\n",
    "print(\"Logged to W&B as artifact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ba37c577-a9e7-4065-89f7-f95b5b9c9f5e",
    "_uuid": "a8b421f5-afe7-4ffb-be3b-4f6eb3cfd321",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Finish wandb run\n",
    "wandb.finish()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training and evaluation completed successfully!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel saved to: llama-3.1-8b-bangla-empathic-lora\")\n",
    "print(f\"Human evaluation samples: human_evaluation_samples.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3497143,
     "sourceId": 6104837,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
