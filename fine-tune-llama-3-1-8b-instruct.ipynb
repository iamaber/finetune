{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6104837,"sourceType":"datasetVersion","datasetId":3497143}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install unsloth\n!pip install evaluate","metadata":{"_cell_guid":"534a6dd0-a490-40e7-9a1d-bcdef943af7a","_uuid":"beba92bc-a60c-4831-9ecf-0a9e3d23db7e","collapsed":false,"execution":{"iopub.status.busy":"2025-11-30T14:10:34.146736Z","iopub.execute_input":"2025-11-30T14:10:34.147005Z","iopub.status.idle":"2025-11-30T14:14:27.159164Z","shell.execute_reply.started":"2025-11-30T14:10:34.146980Z","shell.execute_reply":"2025-11-30T14:14:27.158369Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting unsloth\n  Downloading unsloth-2025.11.4-py3-none-any.whl.metadata (64 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting unsloth_zoo>=2025.11.4 (from unsloth)\n  Downloading unsloth_zoo-2025.11.5-py3-none-any.whl.metadata (32 kB)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (25.0)\nRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.21.0+cu124)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.26.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (7.1.3)\nCollecting tyro (from unsloth)\n  Downloading tyro-0.9.35-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from unsloth) (6.33.0)\nCollecting xformers>=0.0.27.post2 (from unsloth)\n  Downloading xformers-0.0.33.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\nCollecting bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 (from unsloth)\n  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.2.0)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\nCollecting datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1 (from unsloth)\n  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.9.0)\nRequirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.16.0)\nRequirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.36.0)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.34.0)\nRequirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.53.3)\nCollecting trl!=0.19.0,<=0.24.0,>=0.18.2 (from unsloth)\n  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.3)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.20.0)\nCollecting pyarrow>=21.0.0 (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.6.0)\nCollecting multiprocess<0.70.17 (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth)\n  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\nCollecting fsspec<=2025.9.0,>=2023.1.0 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth)\n  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth) (2025.11.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth) (0.21.2)\nCollecting transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3 (from unsloth)\n  Downloading transformers-4.57.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting torchao>=0.13.0 (from unsloth_zoo>=2025.11.4->unsloth)\n  Downloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\nCollecting cut_cross_entropy (from unsloth_zoo>=2025.11.4->unsloth)\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.11.4->unsloth) (11.3.0)\nCollecting msgspec (from unsloth_zoo>=2025.11.4->unsloth)\n  Downloading msgspec-0.20.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\nCollecting torch>=2.4.0 (from unsloth)\n  Downloading torch-2.9.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\nCollecting sympy>=1.13.3 (from torch>=2.4.0->unsloth)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch>=2.4.0->unsloth)\n  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cusparselt-cu12==0.7.1 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\nCollecting nvidia-nccl-cu12==2.27.5 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvshmem-cu12==3.3.20 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\nCollecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting triton>=3.0.0 (from unsloth)\n  Downloading triton-3.5.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.7.0)\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\nCollecting torchvision (from unsloth)\n  Downloading torchvision-0.24.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n  Downloading torchvision-0.24.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.17.0)\nRequirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (14.2.0)\nCollecting shtab>=1.5.6 (from tyro->unsloth)\n  Downloading shtab-1.8.0-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.4)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.16.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.5.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->unsloth) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.22.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.3.1)\nDownloading unsloth-2025.11.4-py3-none-any.whl (358 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m358.7/358.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.57.2-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading unsloth_zoo-2025.11.5-py3-none-any.whl (284 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.4/284.4 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xformers-0.0.33.post1-cp39-abi3-manylinux_2_28_x86_64.whl (122.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.9.0-cp311-cp311-manylinux_2_28_x86_64.whl (899.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m899.8/899.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-3.5.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.4/170.4 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchvision-0.24.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.9.35-py3-none-any.whl (132 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading shtab-1.8.0-py3-none-any.whl (14 kB)\nDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nDownloading msgspec-0.20.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (219 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m219.9/219.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: torchao, nvidia-cusparselt-cu12, triton, sympy, shtab, pyarrow, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multiprocess, msgspec, fsspec, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, tyro, tokenizers, nvidia-cusolver-cu12, torch, cut_cross_entropy, transformers, datasets, trl, xformers, unsloth_zoo, torchvision, bitsandbytes, unsloth\n  Attempting uninstall: torchao\n    Found existing installation: torchao 0.10.0\n    Uninstalling torchao-0.10.0:\n      Successfully uninstalled torchao-0.10.0\n  Attempting uninstall: nvidia-cusparselt-cu12\n    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.18\n    Uninstalling multiprocess-0.70.18:\n      Successfully uninstalled multiprocess-0.70.18\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.10.0\n    Uninstalling fsspec-2025.10.0:\n      Successfully uninstalled fsspec-2025.10.0\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n  Attempting uninstall: datasets\n    Found existing installation: datasets 4.4.1\n    Uninstalling datasets-4.4.1:\n      Successfully uninstalled datasets-4.4.1\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.21.0+cu124\n    Uninstalling torchvision-0.21.0+cu124:\n      Successfully uninstalled torchvision-0.21.0+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ns3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2025.9.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.9.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.48.2 cut_cross_entropy-25.1.1 datasets-4.3.0 fsspec-2025.9.0 msgspec-0.20.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 pyarrow-22.0.0 shtab-1.8.0 sympy-1.14.0 tokenizers-0.22.1 torch-2.9.0 torchao-0.14.1 torchvision-0.24.0 transformers-4.57.2 triton-3.5.0 trl-0.24.0 tyro-0.9.35 unsloth-2025.11.4 unsloth_zoo-2025.11.5 xformers-0.0.33.post1\nCollecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.3.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\nDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.6\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from unsloth import FastLanguageModel, is_bfloat16_supported\nimport torch\nimport pandas as pd\nimport json\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset, DatasetDict\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments","metadata":{"_cell_guid":"a5d473c7-17e6-421f-b9a1-de701a1f1b5a","_uuid":"07e7faad-c27b-4930-b5ea-b55e47f6d5cf","collapsed":false,"execution":{"iopub.status.busy":"2025-11-30T14:14:27.160982Z","iopub.execute_input":"2025-11-30T14:14:27.161228Z","iopub.status.idle":"2025-11-30T14:14:59.284805Z","shell.execute_reply.started":"2025-11-30T14:14:27.161204Z","shell.execute_reply":"2025-11-30T14:14:59.284236Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","text":"ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-30 14:14:33.361725: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764512073.665673      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764512073.711859      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nmy_secret = user_secrets.get_secret(\"wandb_api_key\") \nwandb.login(key=my_secret)\n\nwandb.init(\n    project=\"llama-bangla-empathic\",\n    name=\"llama-3.1-8b-finetuning-v1\",\n    config={\n        \"model\": \"Llama-3.1-8B-Instruct\",\n        \"dataset\": \"bangla-empathic\",\n        \"task\": \"instruction-finetuning\",\n        \"language\": \"bangla\",\n        \"epochs\": 3,\n        \"batch_size\": 2,\n        \"gradient_accumulation_steps\": 8,\n        \"learning_rate\": 2e-4,\n        \"lora_r\": 16,\n        \"lora_alpha\": 16,\n        \"max_seq_length\": 2020,\n    },\n    tags=[\"llama-3.1\", \"bangla\", \"empathic\", \"unsloth\", \"lora\"]\n)","metadata":{"execution":{"iopub.status.busy":"2025-11-30T14:14:59.285469Z","iopub.execute_input":"2025-11-30T14:14:59.285648Z","iopub.status.idle":"2025-11-30T14:15:13.577316Z","shell.execute_reply.started":"2025-11-30T14:14:59.285633Z","shell.execute_reply":"2025-11-30T14:15:13.576661Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maber-islam-dev\u001b[0m (\u001b[33maber-islam-dev-jvai\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251130_141506-w156ioha</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic/runs/w156ioha' target=\"_blank\">llama-3.1-8b-finetuning-v1</a></strong> to <a href='https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic' target=\"_blank\">https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic/runs/w156ioha' target=\"_blank\">https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic/runs/w156ioha</a>"},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic/runs/w156ioha?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7c1c11d63310>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"class DatasetProcessor:\n    # Process and format datasets for fine-tuning\n    \n    def __init__(self, csv_path):\n        self.csv_path = csv_path\n        self.df = None\n        self.train_df = None\n        self.val_df = None\n        self.test_df = None\n        self.train_dataset = None\n        self.val_dataset = None\n        self.test_dataset = None\n    \n    def load_and_clean_data(self):\n        # Load and clean the dataset\n        self.df = pd.read_csv(self.csv_path)\n        \n        # Keep only Questions and Answers columns\n        self.df = self.df[['Questions', 'Answers']].copy()\n        \n        # Remove rows with missing values\n        self.df = self.df.dropna()\n        \n        # Remove rows where text is empty after stripping whitespace\n        self.df = self.df[(self.df['Questions'].str.strip() != '') & (self.df['Answers'].str.strip() != '')]\n        \n        # Strip whitespace\n        self.df['Questions'] = self.df['Questions'].str.strip()\n        self.df['Answers'] = self.df['Answers'].str.strip()\n        \n        print(f\"Dataset size after cleaning: {len(self.df)}\")\n        return self.df\n    \n    def split_data(self, test_size=0.2, val_size=0.5, random_state=42):\n        # Split data into train, validation, and test sets\n        # First split: 80% train, 20% temp (for val + test)\n        self.train_df, temp_df = train_test_split(\n            self.df, test_size=test_size, random_state=random_state, shuffle=True\n        )\n        \n        # Second split: Split temp into 50% validation, 50% test (10% each of total)\n        self.val_df, self.test_df = train_test_split(\n            temp_df, test_size=val_size, random_state=random_state, shuffle=True\n        )\n        \n        print(f\"Training samples:{len(self.train_df)}\")\n        print(f\"Validation samples:{len(self.val_df)}\")\n        print(f\"Test samples:{len(self.test_df)}\")\n        \n        return self.train_df, self.val_df, self.test_df\n    \n    @staticmethod\n    def format_prompt(question, answer=None):\n        # Format prompt using Llama 3.1 official format\n        # Reference: https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/\n        prompt = (\n            \"<|begin_of_text|>\"\n            \"<|start_header_id|>system<|end_header_id|>\\n\\n\"\n            \"You are a sympathetic and helpful assistant. You answer people's questions in Bengali language.<|eot_id|>\"\n            \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n            f\"{question}<|eot_id|>\"\n            \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n        )\n        if answer:\n            prompt += f\"{answer}<|eot_id|>\"\n        return prompt\n    \n    def formatting_prompts_func(self, examples):\n        # Format dataset examples for training\n        questions = examples['Questions']\n        answers = examples['Answers']\n        texts = [self.format_prompt(q, a) for q, a in zip(questions, answers)]\n        return {\"text\": texts}\n    \n    def create_datasets(self):\n        # Create HuggingFace datasets from dataframes\n        self.train_dataset = Dataset.from_pandas(self.train_df[['Questions', 'Answers']].reset_index(drop=True))\n        self.val_dataset = Dataset.from_pandas(self.val_df[['Questions', 'Answers']].reset_index(drop=True))\n        self.test_dataset = Dataset.from_pandas(self.test_df[['Questions', 'Answers']].reset_index(drop=True))\n        \n        # Apply formatting\n        self.train_dataset = self.train_dataset.map(self.formatting_prompts_func, batched=True)\n        self.val_dataset = self.val_dataset.map(self.formatting_prompts_func, batched=True)\n        self.test_dataset = self.test_dataset.map(self.formatting_prompts_func, batched=True)\n        \n        return self.train_dataset, self.val_dataset, self.test_dataset\n    \n    def display_sample(self, num_samples=2):\n        # Display sample formatted data\n        print(\"Sample formatted training data:\")\n        for i in range(min(num_samples, len(self.train_dataset))):\n            print(f\"\\nSample {i+1}:\")\n            print(\"=\"*80)\n            print(self.train_dataset[i][\"text\"])","metadata":{"execution":{"iopub.status.busy":"2025-11-30T14:15:13.578848Z","iopub.execute_input":"2025-11-30T14:15:13.579093Z","iopub.status.idle":"2025-11-30T14:15:13.591540Z","shell.execute_reply.started":"2025-11-30T14:15:13.579068Z","shell.execute_reply":"2025-11-30T14:15:13.590930Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class LLAMAFineTuner:\n    # Fine-tune Llama 3.1 model with LoRA\n    \n    def __init__(self, model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct\", \n                 max_seq_length=2020, dtype=None, load_in_4bit=False, device_map=\"balanced\"):\n        self.model_name = model_name\n        self.max_seq_length = max_seq_length\n        self.dtype = dtype\n        self.load_in_4bit = load_in_4bit\n        self.device_map = device_map\n        self.model = None\n        self.tokenizer = None\n        self.trainer = None\n    \n    def load_model(self):\n        # Load the base model and tokenizer\n        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n            model_name=self.model_name,\n            max_seq_length=self.max_seq_length,\n            dtype=self.dtype,\n            load_in_4bit=self.load_in_4bit,\n            device_map=self.device_map\n        )\n        print(f\"Model loaded: {self.model_name}\")\n        return self.model, self.tokenizer\n    \n    def apply_lora(self, r=16, lora_alpha=16, lora_dropout=0, bias=\"none\",\n                   use_gradient_checkpointing= True, random_state=3407):\n        # Apply LoRA configuration to model\n        self.model = FastLanguageModel.get_peft_model(\n            self.model,\n            r=r, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                          \"gate_proj\", \"up_proj\", \"down_proj\"],\n            lora_alpha=lora_alpha,\n            lora_dropout=lora_dropout, # Supports any, but = 0 is optimized\n            bias=bias, # Supports any, but = \"none\" is optimized\n            # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n            use_gradient_checkpointing=use_gradient_checkpointing, # True or \"unsloth\" for very long context\n            random_state=random_state,\n            use_rslora=False, # We support rank stabilized LoRA\n            loftq_config=None, # And LoftQ\n        )\n        print(\"LoRA configuration applied\")\n        return self.model\n    \n    def create_trainer(self, train_dataset, val_dataset, per_device_train_batch_size=2, \n                       gradient_accumulation_steps=8, num_train_epochs=0.5, learning_rate=2e-4, \n                       output_dir=\"outputs\"):\n        # Create SFT trainer\n        self.trainer = SFTTrainer(\n            model=self.model,\n            tokenizer=self.tokenizer,\n            train_dataset=train_dataset,\n            eval_dataset=val_dataset, # Use validation set for monitoring\n            dataset_text_field=\"text\",\n            max_seq_length=self.max_seq_length,\n            dataset_num_proc=2,\n            packing=False, # Can make training 5x faster for short sequences\n            \n            args=TrainingArguments(\n                per_device_train_batch_size=per_device_train_batch_size,\n                per_device_eval_batch_size=2,\n                gradient_accumulation_steps=gradient_accumulation_steps,\n                warmup_steps=100, # warmup for gradually increasing the learning rate\n                \n                # Choose one: num_train_epochs OR max_steps\n                num_train_epochs=num_train_epochs, # For full training - trains through entire dataset\n                # max_steps=max_steps, # Or use this for quick testing\n                learning_rate=learning_rate, \n                fp16=not is_bfloat16_supported(),\n                bf16=is_bfloat16_supported(),\n                logging_steps=10,\n                optim=\"adamw_8bit\",\n                weight_decay=0.01,\n                lr_scheduler_type=\"cosine\",\n                seed=42,\n                output_dir=output_dir,\n                \n                # Evaluation and checkpointing\n                eval_strategy=\"steps\", # Changed from evaluation_strategy\n                eval_steps=50, # Evaluate less frequently to speed up training\n                save_strategy=\"steps\",\n                save_steps=100, # Save less frequently\n                save_total_limit=3, # Keep only best 3 checkpoints\n                load_best_model_at_end=True,\n                metric_for_best_model=\"eval_loss\",\n                greater_is_better=False,\n                \n                # Weights & Biases integration\n                report_to=\"wandb\", # Enable wandb logging\n                run_name=\"llama-3.2-8b-finetuning-v1\", # Run name in wandb\n                logging_first_step=True,\n                logging_nan_inf_filter=True,\n                remove_unused_columns=False, # Keep all columns for SFTTrainer\n            ),\n        )\n        print(\"Trainer configured\")\n        return self.trainer\n    \n    def train(self):\n        # Train the model\n        from unsloth import unsloth_train\n        trainer_stats = unsloth_train(self.trainer) # unsloth_train fixes gradient_accumulation_steps\n        print(\"Training completed\")\n        return trainer_stats\n    \n    def save_model(self, output_dir=\"llama-3.1-8b-bangla-empathic-lora\"):\n        # Save the fine-tuned model\n        self.model.save_pretrained(output_dir)\n        self.tokenizer.save_pretrained(output_dir)\n        # save to merged 16bit\n        self.model.save_pretrained_merged(\"llama-3.1-8b-bangla-empathic-merged\", self.tokenizer, save_method=\"merged_16bit\")\n        print(f\"Model saved to: {output_dir}\")\n    \n    def enable_inference_mode(self):\n        # Enable inference mode for the model\n        FastLanguageModel.for_inference(self.model)\n","metadata":{"execution":{"iopub.status.busy":"2025-11-30T14:15:13.592444Z","iopub.execute_input":"2025-11-30T14:15:13.592691Z","iopub.status.idle":"2025-11-30T14:15:13.678228Z","shell.execute_reply.started":"2025-11-30T14:15:13.592669Z","shell.execute_reply":"2025-11-30T14:15:13.677570Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Evaluator:\n    # Evaluate fine-tuned model\n    \n    def __init__(self, fine_tuner, data_processor):\n        self.model = fine_tuner.model\n        self.tokenizer = fine_tuner.tokenizer\n        self.trainer = fine_tuner.trainer\n        self.test_df = data_processor.test_df\n        self.test_dataset = data_processor.test_dataset\n        self.format_prompt = data_processor.format_prompt\n    \n    def generate_response(self, question, max_new_tokens=256, temperature=0.7, top_p=0.9):\n        # Generate a response for a question\n        prompt = self.format_prompt(question)\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n        \n        outputs = self.model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            use_cache=True\n        )\n        \n        # Decode the full output and extract only the response\n        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # Remove the prompt from the response\n        # Find the assistant's response after the last header\n        if \"<|start_header_id|>assistant<|end_header_id|>\" in response:\n            response = response.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].strip()\n        \n        return response\n    \n    def evaluate_metrics(self, num_samples=100):\n        # Evaluate model with BLEU, ROUGE, and Perplexity\n        import numpy as np\n        from evaluate import load\n        \n        # Load evaluation metrics\n        bleu_metric = load(\"bleu\")\n        rouge_metric = load(\"rouge\")\n        \n        # Generate predictions on test set\n        predictions = []\n        references = []\n        \n        print(f\"Generating predictions for {min(num_samples, len(self.test_dataset))} samples...\")\n        for i in range(min(num_samples, len(self.test_dataset))):\n            question = self.test_df.iloc[i]['Questions']\n            reference = self.test_df.iloc[i]['Answers']\n            prediction = self.generate_response(question)\n            predictions.append(prediction)\n            references.append(reference)\n            \n            if (i + 1) % 5 == 0:\n                print(f\"  Generated {i + 1}/{min(num_samples, len(self.test_dataset))} predictions...\")\n        \n        # Calculate BLEU score\n        bleu_results = bleu_metric.compute(predictions=predictions, references=[[ref] for ref in references])\n        \n        # Calculate ROUGE scores\n        rouge_results = rouge_metric.compute(predictions=predictions, references=references)\n        \n        # Calculate perplexity from evaluation loss\n        print(\"Calculating perplexity...\")\n        eval_results = self.trainer.evaluate(eval_dataset=self.test_dataset)\n        perplexity = np.exp(eval_results['eval_loss'])\n        \n        results = {\n            \"perplexity\": perplexity,\n            \"bleu\": bleu_results['bleu'],\n            \"rouge1\": rouge_results['rouge1'],\n            \"rouge2\": rouge_results['rouge2'],\n            \"rougeL\": rouge_results['rougeL']\n        }\n        \n        print(f\"\\nEvaluation Results:\")\n        print(f\"Perplexity: {results['perplexity']:.4f}\")\n        print(f\"BLEU Score: {results['bleu']:.4f}\")\n        print(f\"ROUGE-1: {results['rouge1']:.4f}\")\n        print(f\"ROUGE-2: {results['rouge2']:.4f}\")\n        print(f\"ROUGE-L: {results['rougeL']:.4f}\")\n        \n        return results\n    \n    def create_human_eval_samples(self, sample_size=20, output_file=\"human_evaluation_samples.csv\"):\n        # Create samples for human evaluation\n        import random\n        \n        # Sample random examples for human evaluation\n        sample_indices = random.sample(range(len(self.test_df)), min(sample_size, len(self.test_df)))\n        human_eval_data = []\n        \n        for idx in sample_indices:\n            question = self.test_df.iloc[idx]['Questions']\n            reference = self.test_df.iloc[idx]['Answers']\n            \n            # Get model prediction\n            prediction = self.generate_response(question)\n            \n            human_eval_data.append({\n                \"id\": idx,\n                \"question\": question,\n                \"reference_answer\": reference,\n                \"model_answer\": prediction,\n                \"empathy_score\": None, # To be filled by human evaluators (1-5)\n                \"relevance_score\": None, # To be filled by human evaluators (1-5)\n                \"fluency_score\": None, # To be filled by human evaluators (1-5)\n                \"notes\": \"\" # Additional comments\n            })\n        \n        # Save to CSV for human evaluation\n        human_eval_df = pd.DataFrame(human_eval_data)\n        human_eval_df.to_csv(output_file, index=False)\n        \n        print(f\"\\nCreated {len(human_eval_data)} samples for human evaluation\")\n        print(f\"Saved to: {output_file}\")\n        print(\"\\nEvaluation criteria:\")\n        print(\"1. Empathy Score (1-5): How empathetic and understanding is the response?\")\n        print(\"2. Relevance Score (1-5): How relevant is the response to the question?\")\n        print(\"3. Fluency Score (1-5): How fluent and natural is the Bengali language?\")\n        \n        return human_eval_df\n    \n    def display_sample_responses(self, num_samples=5):\n        # Display sample responses with streaming\n        from transformers import TextStreamer\n        print(f\"Generating {num_samples} sample responses on test prompts\")\n       \n        \n        for i in range(min(num_samples, len(self.test_df))):\n            question = self.test_df.iloc[i]['Questions']\n            reference = self.test_df.iloc[i]['Answers']\n            \n            print(f\"\\n--- Sample {i+1} ---\")\n            print(f\"Question: {question}\")\n            print(f\"\\nReference Answer: {reference}\")\n            print(f\"\\nModel Response:\")\n            \n            # Create prompt\n            prompt = self.format_prompt(question)\n            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n            \n            # Generate with streaming\n            text_streamer = TextStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)\n            _ = self.model.generate(\n                **inputs,\n                streamer=text_streamer,\n                max_new_tokens=300,\n                temperature=0.5,\n                top_p=0.9,\n                use_cache=True\n            )\n            \n            print(\"\\n\" + \"-\"*80)\n    \n    def log_all_responses(self, experiment_name=\"llama-3.1-8b-bangla-empathic\"):\n        # Log all test responses\n        from datetime import datetime\n        import uuid\n        \n        # Generate experiment ID\n        experiment_id = f\"{experiment_name}-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n        \n        # Generate responses for all test samples and log them\n        generated_responses = []\n        \n        print(f\"Generating and logging responses for {len(self.test_df)} test samples...\")\n        \n        for idx in range(len(self.test_df)):\n            question = self.test_df.iloc[idx]['Questions']\n            reference = self.test_df.iloc[idx]['Answers']\n            response_text = self.generate_response(question)\n            \n            # Create log entry\n            log_entry = {\n                \"id\": str(uuid.uuid4()),\n                \"experiment_id\": experiment_id,\n                \"sample_index\": idx,\n                \"input_text\": question,\n                \"reference_text\": reference,\n                \"response_text\": response_text,\n                \"timestamp\": datetime.now().isoformat(),\n                \"model_name\": \"llama-3.1-8b-instruct\",\n                \"temperature\": 0.5,\n                \"top_p\": 0.9,\n                \"max_new_tokens\": 300\n            }\n            \n            generated_responses.append(log_entry)\n            \n            if (idx + 1) % 10 == 0:\n                print(f\"Processed {idx + 1}/{len(self.test_df)} samples...\")\n        \n        # Save to CSV\n        responses_df = pd.DataFrame(generated_responses)\n        log_filename = f\"generated_responses_{experiment_id}.csv\"\n        responses_df.to_csv(log_filename, index=False)\n        \n        print(f\"\\nGenerated responses logged successfully!\")\n        print(f\"Total responses: {len(generated_responses)}\")\n        print(f\"Saved to: {log_filename}\")\n        \n        return responses_df, log_filename","metadata":{"execution":{"iopub.status.busy":"2025-11-30T14:15:13.679048Z","iopub.execute_input":"2025-11-30T14:15:13.679305Z","iopub.status.idle":"2025-11-30T14:15:13.697581Z","shell.execute_reply.started":"2025-11-30T14:15:13.679281Z","shell.execute_reply":"2025-11-30T14:15:13.696956Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"print(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU: {torch.cuda.get_device_name(1)}\")\n    print(\n        f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} + {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n    )","metadata":{"_cell_guid":"eb9412e3-a0ae-4091-bb13-9de72737823a","_uuid":"3b8a32a3-b074-4767-867f-411a4a343ff6","collapsed":false,"execution":{"iopub.status.busy":"2025-11-30T14:15:13.699508Z","iopub.execute_input":"2025-11-30T14:15:13.699884Z","iopub.status.idle":"2025-11-30T14:15:13.712891Z","shell.execute_reply.started":"2025-11-30T14:15:13.699865Z","shell.execute_reply":"2025-11-30T14:15:13.712190Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","text":"PyTorch version: 2.9.0+cu128\nCUDA available: True\nGPU: Tesla T4\nGPU: Tesla T4\nGPU Memory: 15.8 + 15.8 GB\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Initialize DatasetProcessor\ndata_processor = DatasetProcessor('/kaggle/input/bengali-empathetic-conversations-corpus/BengaliEmpatheticConversationsCorpus .csv')\n\n# Load and clean data\ndf = data_processor.load_and_clean_data()\ndf.head()","metadata":{"_cell_guid":"d0da5b53-af13-4ce5-80b2-78d6cd56251d","_uuid":"eb274f00-f6e9-4d7f-a013-228cbf3b23b1","collapsed":false,"execution":{"iopub.status.busy":"2025-11-30T14:15:13.713967Z","iopub.execute_input":"2025-11-30T14:15:13.714594Z","iopub.status.idle":"2025-11-30T14:15:14.267537Z","shell.execute_reply.started":"2025-11-30T14:15:13.714568Z","shell.execute_reply":"2025-11-30T14:15:14.266962Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","text":"Dataset size after cleaning: 38210\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                           Questions  \\\n0  à¦†à¦®à¦¾à¦° à¦¸à§à¦¤à§à¦°à§€ à¦à¦¬à¦‚ à¦®à¦¾à¦¯à¦¼à§‡à¦° à¦®à¦§à§à¦¯à§‡ à¦Ÿà¦¾à¦¨à¦Ÿà¦¾à¦¨ à¦®à¦¤à¦¬à¦¿à¦°à§‹à¦§ à¦šà¦²...   \n1  à¦†à¦®à¦¿ à¦¬à¦¾à¦šà§à¦šà¦¾ à¦¨à§‡à¦“à¦¯à¦¼à¦¾à¦° à¦ªà¦°à¦¿à¦•à¦²à§à¦ªà¦¨à¦¾ à¦•à¦°à¦›à¦¿, à¦¤à¦¾à¦‡ à¦†à¦®à¦¾à¦•à§‡ à¦§...   \n2  à¦†à¦®à¦¾à¦° à¦®à¦¨à§‡à¦° à¦®à¦§à§à¦¯à§‡ à¦—à§‹à¦ªà¦¨ à¦†à¦›à§‡, à¦à¦¬à¦‚ à¦†à¦®à¦¿ à¦œà¦¾à¦¨à¦¿ à¦¨à¦¾ à¦¤à¦¾à¦¦à§‡...   \n3  à¦†à¦®à¦¿ à¦†à¦®à¦¾à¦° à¦¸à¦®à§à¦ªà¦°à§à¦•à§‡à¦° à¦•à§à¦·à§‡à¦¤à§à¦°à§‡ à¦…à¦¤à§à¦¯à¦¨à§à¦¤ à¦…à¦§à¦¿à¦•à¦¾à¦°à¦¸à§‚à¦šà¦•...   \n4  à¦•à¦¯à¦¼à§‡à¦• à¦¬à¦›à¦° à¦†à¦—à§‡ à¦†à¦®à¦¾à¦° à¦®à¦¾à¦¥à¦¾à¦¯à¦¼ à¦†à¦˜à¦¾à¦¤ à¦²à§‡à¦—à§‡à¦›à¦¿à¦² à¦à¦¬à¦‚ à¦†à¦®à¦¾...   \n\n                                             Answers  \n0  à¦†à¦ªà¦¨à¦¿ à¦¯à¦¾ à¦¬à¦°à§à¦£à¦¨à¦¾ à¦•à¦°à¦›à§‡à¦¨ à¦¤à¦¾à¦•à§‡ à¦®à¦¨à§‹à¦¬à¦¿à¦œà§à¦à¦¾à¦¨à§€à¦°à¦¾ \"à¦¤à§à¦°à¦¿à¦­...  \n1  à¦¹à¦¾à¦‡à¥¤ à¦†à¦ªà¦¨à¦¾à¦° à¦¶à¦¿à¦¶à§à¦° (à¦à¦¬à¦‚ à¦¨à¦¿à¦œà§‡à¦°) à¦œà¦¨à§à¦¯ à¦¯à¦¾ à¦¸à§à¦¬à¦¾à¦¸à§à¦¥à§à¦¯...  \n2  à¦®à¦¨à§‡ à¦¹à¦šà§à¦›à§‡ à¦—à§‹à¦ªà¦¨ à¦°à¦¾à¦–à¦¾ à¦à¦–à¦¨ à¦†à¦ªà¦¨à¦¾à¦° à¦œà¦¨à§à¦¯ à¦à¦•à¦Ÿà¦¿ à¦¸à¦®à¦¸à§à¦¯à¦¾...  \n3  à¦¹à§à¦¯à¦¾à¦²à§‹à¥¤ à¦à¦Ÿà¦¾ à¦¦à§à¦°à§à¦¦à¦¾à¦¨à§à¦¤ à¦¯à§‡ à¦†à¦ªà¦¨à¦¿ à¦‰à¦ªà¦²à¦¬à§à¦§à¦¿ à¦•à¦°à¦¤à§‡ à¦¸à¦•à§...  \n4  à¦†à¦ªà¦¨à¦¿ à¦¬à¦²à§‡à¦¨à¦¨à¦¿ à¦•à¦¿ à¦¬à¦¾ à¦•à¦¤ à¦“à¦·à§à¦§ à¦†à¦ªà¦¨à¦¿ à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à§‡à¦›à§‡à¦¨à¥¤ ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Questions</th>\n      <th>Answers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>à¦†à¦®à¦¾à¦° à¦¸à§à¦¤à§à¦°à§€ à¦à¦¬à¦‚ à¦®à¦¾à¦¯à¦¼à§‡à¦° à¦®à¦§à§à¦¯à§‡ à¦Ÿà¦¾à¦¨à¦Ÿà¦¾à¦¨ à¦®à¦¤à¦¬à¦¿à¦°à§‹à¦§ à¦šà¦²...</td>\n      <td>à¦†à¦ªà¦¨à¦¿ à¦¯à¦¾ à¦¬à¦°à§à¦£à¦¨à¦¾ à¦•à¦°à¦›à§‡à¦¨ à¦¤à¦¾à¦•à§‡ à¦®à¦¨à§‹à¦¬à¦¿à¦œà§à¦à¦¾à¦¨à§€à¦°à¦¾ \"à¦¤à§à¦°à¦¿à¦­...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>à¦†à¦®à¦¿ à¦¬à¦¾à¦šà§à¦šà¦¾ à¦¨à§‡à¦“à¦¯à¦¼à¦¾à¦° à¦ªà¦°à¦¿à¦•à¦²à§à¦ªà¦¨à¦¾ à¦•à¦°à¦›à¦¿, à¦¤à¦¾à¦‡ à¦†à¦®à¦¾à¦•à§‡ à¦§...</td>\n      <td>à¦¹à¦¾à¦‡à¥¤ à¦†à¦ªà¦¨à¦¾à¦° à¦¶à¦¿à¦¶à§à¦° (à¦à¦¬à¦‚ à¦¨à¦¿à¦œà§‡à¦°) à¦œà¦¨à§à¦¯ à¦¯à¦¾ à¦¸à§à¦¬à¦¾à¦¸à§à¦¥à§à¦¯...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>à¦†à¦®à¦¾à¦° à¦®à¦¨à§‡à¦° à¦®à¦§à§à¦¯à§‡ à¦—à§‹à¦ªà¦¨ à¦†à¦›à§‡, à¦à¦¬à¦‚ à¦†à¦®à¦¿ à¦œà¦¾à¦¨à¦¿ à¦¨à¦¾ à¦¤à¦¾à¦¦à§‡...</td>\n      <td>à¦®à¦¨à§‡ à¦¹à¦šà§à¦›à§‡ à¦—à§‹à¦ªà¦¨ à¦°à¦¾à¦–à¦¾ à¦à¦–à¦¨ à¦†à¦ªà¦¨à¦¾à¦° à¦œà¦¨à§à¦¯ à¦à¦•à¦Ÿà¦¿ à¦¸à¦®à¦¸à§à¦¯à¦¾...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>à¦†à¦®à¦¿ à¦†à¦®à¦¾à¦° à¦¸à¦®à§à¦ªà¦°à§à¦•à§‡à¦° à¦•à§à¦·à§‡à¦¤à§à¦°à§‡ à¦…à¦¤à§à¦¯à¦¨à§à¦¤ à¦…à¦§à¦¿à¦•à¦¾à¦°à¦¸à§‚à¦šà¦•...</td>\n      <td>à¦¹à§à¦¯à¦¾à¦²à§‹à¥¤ à¦à¦Ÿà¦¾ à¦¦à§à¦°à§à¦¦à¦¾à¦¨à§à¦¤ à¦¯à§‡ à¦†à¦ªà¦¨à¦¿ à¦‰à¦ªà¦²à¦¬à§à¦§à¦¿ à¦•à¦°à¦¤à§‡ à¦¸à¦•à§...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>à¦•à¦¯à¦¼à§‡à¦• à¦¬à¦›à¦° à¦†à¦—à§‡ à¦†à¦®à¦¾à¦° à¦®à¦¾à¦¥à¦¾à¦¯à¦¼ à¦†à¦˜à¦¾à¦¤ à¦²à§‡à¦—à§‡à¦›à¦¿à¦² à¦à¦¬à¦‚ à¦†à¦®à¦¾...</td>\n      <td>à¦†à¦ªà¦¨à¦¿ à¦¬à¦²à§‡à¦¨à¦¨à¦¿ à¦•à¦¿ à¦¬à¦¾ à¦•à¦¤ à¦“à¦·à§à¦§ à¦†à¦ªà¦¨à¦¿ à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à§‡à¦›à§‡à¦¨à¥¤ ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Initialize LLAMAFineTuner\nfine_tuner = LLAMAFineTuner(\n    model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct\",\n    max_seq_length=2020, # Choose any! We auto support RoPE Scaling internally!\n    dtype=None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n    load_in_4bit=False, # Use 4-bit quantization to reduce memory usage. Can be False.\n    device_map=\"balanced\" # Enables Multi-GPU Training\n)\n\n# Load model and tokenizer\nmodel, tokenizer = fine_tuner.load_model()","metadata":{"_cell_guid":"626a8440-adcc-4b5c-9d08-630b7675839b","_uuid":"b00230af-b065-4d04-aa16-104b78ca85d8","collapsed":false,"execution":{"iopub.status.busy":"2025-11-30T14:15:14.268329Z","iopub.execute_input":"2025-11-30T14:15:14.268673Z","iopub.status.idle":"2025-11-30T14:16:56.080115Z","shell.execute_reply.started":"2025-11-30T14:15:14.268647Z","shell.execute_reply":"2025-11-30T14:16:56.079276Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20011714709a47b1a8e355e992494367"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63fc4d0d371f44e598c874e90a348721"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecd9f31d49f34db7a33061b8b4eaf949"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2291372037547659c86058424ffe9b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bef50e16018d4eea8019239eb860ff0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7609d0a53a14b228663dd90631a753f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa912c42a43a400aa5c8d9cd1aee6f73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51868dc1c9bf4bcfa5032c1916a66046"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ed5e9a1e1e04e67971b8dc93b46e126"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"187b64bba5d6464ca7f63a4f7fbb5766"}},"metadata":{}},{"name":"stdout","text":"Model loaded: unsloth/Meta-Llama-3.1-8B-Instruct\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Train/Validation/Test Splits\ntrain_df, val_df, test_df = data_processor.split_data(test_size=0.2, val_size=0.5, random_state=42)","metadata":{"_cell_guid":"d1eb6ba2-0f8e-4aeb-9eaa-0162b75ab440","_uuid":"960fd236-32a8-4507-ad68-ee2d6520f0ef","collapsed":false,"execution":{"iopub.status.busy":"2025-11-30T14:16:56.081102Z","iopub.execute_input":"2025-11-30T14:16:56.081363Z","iopub.status.idle":"2025-11-30T14:16:56.098434Z","shell.execute_reply.started":"2025-11-30T14:16:56.081340Z","shell.execute_reply":"2025-11-30T14:16:56.097591Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","text":"Training samples:30568\nValidation samples:3821\nTest samples:3821\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# STEP 4: CREATE HUGGINGFACE DATASETS\ntrain_dataset, val_dataset, test_dataset = data_processor.create_datasets()","metadata":{"_cell_guid":"4ff7045c-791b-426b-b291-87c30bc7de80","_uuid":"1d0ca9d9-6ab8-46a8-9440-1a9e52210a53","collapsed":false,"execution":{"iopub.status.busy":"2025-11-30T14:16:56.099079Z","iopub.execute_input":"2025-11-30T14:16:56.099244Z","iopub.status.idle":"2025-11-30T14:17:02.794341Z","shell.execute_reply.started":"2025-11-30T14:16:56.099231Z","shell.execute_reply":"2025-11-30T14:17:02.793599Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/30568 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1de0aa504e84cad8a7240c4638e9a4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb316b86719c4542b9f590c2776972a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce08fe7783e74dc4a758fe143f85a043"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Display sample data\ndata_processor.display_sample(num_samples=2)","metadata":{"execution":{"iopub.status.busy":"2025-11-30T14:17:02.795057Z","iopub.execute_input":"2025-11-30T14:17:02.795359Z","iopub.status.idle":"2025-11-30T14:17:02.801468Z","shell.execute_reply.started":"2025-11-30T14:17:02.795342Z","shell.execute_reply":"2025-11-30T14:17:02.800757Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Sample formatted training data:\n\nSample 1:\n================================================================================\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a sympathetic and helpful assistant. You answer people's questions in Bengali language.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nà¦†à¦®à¦¿ à¦†à¦—à¦¾à¦®à§€ à¦«à§‡à¦¬à§à¦°à§à¦¯à¦¼à¦¾à¦°à¦¿à¦¤à§‡ à¦ªà§à¦°à¦®à§‹à¦¶à¦¨ à¦ªà¦¾à¦“à¦¯à¦¼à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à¦›à¦¿à¥¤<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nà¦†à¦°à§‡à¥¤ à¦†à¦®à¦¿ à¦†à¦ªà¦¨à¦¿ à¦à¦Ÿà¦¾ à¦ªà§‡à¦¤à§‡ à¦†à¦¶à¦¾ à¦•à¦°à¦¿!<|eot_id|>\n\nSample 2:\n================================================================================\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a sympathetic and helpful assistant. You answer people's questions in Bengali language.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nà¦†à¦®à¦¿ à¦¨à¦¿à¦œà§‡à¦•à§‡ à¦°à¦•à§à¦·à¦¾ à¦•à¦°à§‡à¦›à¦¿<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nà¦à¦Ÿà¦¾ à¦šà¦®à§à¦•à¦¾à¦°, à¦†à¦®à¦¿ à¦†à¦¶à¦¾ à¦•à¦°à¦¿ à¦†à¦ªà¦¨à¦¿ à¦†à¦˜à¦¾à¦¤ à¦ªà¦¾à¦¬à§‡à¦¨ à¦¨à¦¾!<|eot_id|>\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Apply LoRA\nmodel = fine_tuner.apply_lora(\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=True,\n    random_state=2020\n)","metadata":{"_cell_guid":"5278f42b-88f6-41d8-ab13-6b5de7c292aa","_uuid":"7fc8fa98-3b0f-4126-9601-f1747d95e75e","collapsed":false,"execution":{"iopub.status.busy":"2025-11-30T14:17:02.802324Z","iopub.execute_input":"2025-11-30T14:17:02.802579Z","iopub.status.idle":"2025-11-30T14:17:06.789887Z","shell.execute_reply.started":"2025-11-30T14:17:02.802557Z","shell.execute_reply":"2025-11-30T14:17:06.789216Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stderr","text":"Unsloth 2025.11.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"},{"name":"stdout","text":"LoRA configuration applied\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Create trainer\nfine_tuner.create_trainer(\n    train_dataset=train_dataset,\n    val_dataset=val_dataset,\n)","metadata":{"_cell_guid":"b22387d9-853c-4a3b-8bf3-d8696c98021b","_uuid":"625bdee0-6722-4ae1-9883-913bccb6b12a","collapsed":false,"execution":{"iopub.status.busy":"2025-11-30T14:17:06.790750Z","iopub.execute_input":"2025-11-30T14:17:06.791028Z","iopub.status.idle":"2025-11-30T14:17:24.561723Z","shell.execute_reply.started":"2025-11-30T14:17:06.791010Z","shell.execute_reply":"2025-11-30T14:17:24.560891Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/30568 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47a64b84ef704822bcd5732bac850bdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/3821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89ce84c2a1dc4625949ecb3dd46b371a"}},"metadata":{}},{"name":"stdout","text":"Trainer configured\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<UnslothSFTTrainer.UnslothSFTTrainer at 0x7c1c10207990>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Train the model\ntrainer_stats = fine_tuner.train()","metadata":{"_cell_guid":"14ff21a4-bc89-4e07-9b5d-a64449219000","_uuid":"ae5c4cd4-8a7f-4ede-bc10-27d5add88609","collapsed":false,"execution":{"iopub.status.busy":"2025-11-30T14:17:24.563363Z","iopub.execute_input":"2025-11-30T14:17:24.563585Z","iopub.status.idle":"2025-12-01T00:43:35.382715Z","shell.execute_reply.started":"2025-11-30T14:17:24.563564Z","shell.execute_reply":"2025-12-01T00:43:35.381955Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 2\n   \\\\   /|    Num examples = 30,568 | Num Epochs = 1 | Total steps = 956\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='956' max='956' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [956/956 10:25:36, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.714000</td>\n      <td>0.690269</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.605500</td>\n      <td>0.621667</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.605100</td>\n      <td>0.607307</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.612200</td>\n      <td>0.599261</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.601600</td>\n      <td>0.594432</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.581600</td>\n      <td>0.587140</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.600500</td>\n      <td>0.582795</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.578800</td>\n      <td>0.578181</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.586500</td>\n      <td>0.576018</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.586500</td>\n      <td>0.569828</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.581600</td>\n      <td>0.568041</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.558200</td>\n      <td>0.563291</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.568800</td>\n      <td>0.560052</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.572500</td>\n      <td>0.557277</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.557200</td>\n      <td>0.555080</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.553700</td>\n      <td>0.553631</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.564100</td>\n      <td>0.552408</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.546200</td>\n      <td>0.551981</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.570100</td>\n      <td>0.551860</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\nUsing gradient accumulation will be very slightly less accurate.\nRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–</td></tr><tr><td>eval/runtime</td><td>â–â–ƒâ–ƒâ–â–…â–â–‡â–â–‡â–â–†â–‚â–ƒâ–ƒâ–‚â–‡â–‚â–ˆâ–‚</td></tr><tr><td>eval/samples_per_second</td><td>â–ˆâ–†â–†â–ˆâ–„â–ˆâ–‚â–ˆâ–‚â–ˆâ–ƒâ–‡â–†â–†â–‡â–‚â–‡â–â–‡</td></tr><tr><td>eval/steps_per_second</td><td>â–ˆâ–†â–†â–ˆâ–„â–ˆâ–ƒâ–ˆâ–ƒâ–ˆâ–ƒâ–ˆâ–†â–†â–ˆâ–‚â–‡â–â–‡</td></tr><tr><td>train/epoch</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–‚â–â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–ƒâ–‚â–â–ƒ</td></tr><tr><td>train/learning_rate</td><td>â–â–„â–„â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–„â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.55186</td></tr><tr><td>eval/runtime</td><td>1297.8614</td></tr><tr><td>eval/samples_per_second</td><td>2.944</td></tr><tr><td>eval/steps_per_second</td><td>1.472</td></tr><tr><td>total_flos</td><td>2.0535633812516045e+17</td></tr><tr><td>train/epoch</td><td>0.50039</td></tr><tr><td>train/global_step</td><td>956</td></tr><tr><td>train/grad_norm</td><td>0.21862</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5701</td></tr><tr><td>train_loss</td><td>0.60728</td></tr><tr><td>train_runtime</td><td>37567.9045</td></tr><tr><td>train_samples_per_second</td><td>0.407</td></tr><tr><td>train_steps_per_second</td><td>0.025</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">llama-3.1-8b-finetuning-v1</strong> at: <a href='https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic/runs/w156ioha' target=\"_blank\">https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic/runs/w156ioha</a><br> View project at: <a href='https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic' target=\"_blank\">https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251130_141506-w156ioha/logs</code>"},"metadata":{}},{"name":"stdout","text":"Training completed\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Save model\nfine_tuner.save_model(\"llama-3.1-8b-bangla-empathic-lora\")","metadata":{"_cell_guid":"bf6f273a-dd64-4886-98e1-28f1f6056c09","_uuid":"b3f5e14f-4adc-4c03-a051-e098032b11f5","collapsed":false,"execution":{"iopub.status.busy":"2025-12-01T00:43:35.383694Z","iopub.execute_input":"2025-12-01T00:43:35.384036Z","iopub.status.idle":"2025-12-01T00:46:51.153553Z","shell.execute_reply.started":"2025-12-01T00:43:35.384018Z","shell.execute_reply":"2025-12-01T00:46:51.152740Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","text":"Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\nChecking cache directory for required files...\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: Copying 4 files from cache to `llama-3.1-8b-bangla-empathic-merged`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:00<00:00, 15.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"Successfully copied all 4 files from cache to `llama-3.1-8b-bangla-empathic-merged`\nChecking cache directory for required files...\nCache check failed: tokenizer.model not found in local cache.\nNot all required files found in cache. Will proceed with downloading.\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 33354.31it/s]\nUnsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:12<00:00, 33.11s/it]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merge process complete. Saved to `/kaggle/working/llama-3.1-8b-bangla-empathic-merged`\nModel saved to: llama-3.1-8b-bangla-empathic-lora\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# # Enable inference mode and initialize Evaluator\n# fine_tuner.enable_inference_mode()\n# evaluator = Evaluator(fine_tuner, data_processor)\n\n# # Evaluate using: Perplexity, BLEU, ROUGE\n# results = evaluator.evaluate_metrics(num_samples=10)\n\n# # Log to wandb\n# wandb.log(results)","metadata":{"_cell_guid":"0b7c67b3-6a4e-4cf5-b071-82c3541f238b","_uuid":"4a6f28e0-d3bd-4736-90bc-7a574245844b","collapsed":false,"execution":{"iopub.status.busy":"2025-12-01T00:52:40.049200Z","iopub.execute_input":"2025-12-01T00:52:40.049515Z","iopub.status.idle":"2025-12-01T00:52:40.053390Z","shell.execute_reply.started":"2025-12-01T00:52:40.049495Z","shell.execute_reply":"2025-12-01T00:52:40.052453Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Sample model responses on test prompts\n# evaluator.display_sample_responses(num_samples=3)","metadata":{"_cell_guid":"b160cb3a-68c6-4cda-90dd-531d6c958d4b","_uuid":"6398a7d1-4c40-458f-980f-b2a6ce76399e","collapsed":false,"execution":{"iopub.status.busy":"2025-12-01T00:53:43.412257Z","iopub.execute_input":"2025-12-01T00:53:43.412828Z","iopub.status.idle":"2025-12-01T00:53:43.415872Z","shell.execute_reply.started":"2025-12-01T00:53:43.412804Z","shell.execute_reply":"2025-12-01T00:53:43.415131Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Store logs for GeneratedResponses: id, experiment_id, input_text, response_text, timestamp\n# responses_df, log_filename = evaluator.log_all_responses()\n\n# # Upload to wandb as artifact\n# artifact = wandb.Artifact(\n#     name=\"generated-responses\",\n#     type=\"predictions\",\n#     description=\"Generated responses from fine-tuned model on test set\"\n# )\n# artifact.add_file(log_filename)\n# wandb.log_artifact(artifact)","metadata":{"_cell_guid":"1fa3e950-b4ff-460b-8000-f92145bf0241","_uuid":"a5ca8ca0-87e8-43ac-b2b4-6634f7569ca2","collapsed":false,"execution":{"iopub.status.busy":"2025-12-01T00:53:36.864003Z","iopub.execute_input":"2025-12-01T00:53:36.864726Z","iopub.status.idle":"2025-12-01T00:53:36.867954Z","shell.execute_reply.started":"2025-12-01T00:53:36.864704Z","shell.execute_reply":"2025-12-01T00:53:36.867183Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":25}]}