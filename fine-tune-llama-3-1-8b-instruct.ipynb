{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_cell_guid": "534a6dd0-a490-40e7-9a1d-bcdef943af7a",
    "_uuid": "beba92bc-a60c-4831-9ecf-0a9e3d23db7e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T12:05:25.048107Z",
     "iopub.status.busy": "2025-11-29T12:05:25.047819Z",
     "iopub.status.idle": "2025-11-29T12:05:25.051591Z",
     "shell.execute_reply": "2025-11-29T12:05:25.050914Z",
     "shell.execute_reply.started": "2025-11-29T12:05:25.048086Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install unsloth\n",
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "a5d473c7-17e6-421f-b9a1-de701a1f1b5a",
    "_uuid": "07e7faad-c27b-4930-b5ea-b55e47f6d5cf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T12:05:25.052885Z",
     "iopub.status.busy": "2025-11-29T12:05:25.052663Z",
     "iopub.status.idle": "2025-11-29T12:05:25.074163Z",
     "shell.execute_reply": "2025-11-29T12:05:25.073535Z",
     "shell.execute_reply.started": "2025-11-29T12:05:25.052869Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T12:05:25.075594Z",
     "iopub.status.busy": "2025-11-29T12:05:25.075325Z",
     "iopub.status.idle": "2025-11-29T12:05:34.125859Z",
     "shell.execute_reply": "2025-11-29T12:05:34.125075Z",
     "shell.execute_reply.started": "2025-11-29T12:05:25.075572Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20251129_120525-9vt2ptt4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic/runs/9vt2ptt4' target=\"_blank\">llama-3.1-8b-finetuning-v2</a></strong> to <a href='https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic' target=\"_blank\">https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic/runs/9vt2ptt4' target=\"_blank\">https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic/runs/9vt2ptt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aber-islam-dev-jvai/llama-bangla-empathic/runs/9vt2ptt4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x78546573c550>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "my_secret = user_secrets.get_secret(\"wandb_api_key\") \n",
    "wandb.login(key=my_secret)\n",
    "\n",
    "wandb.init(\n",
    "    project=\"llama-bangla-empathic\",\n",
    "    name=\"llama-3.1-8b-finetuning-v2\",\n",
    "    config={\n",
    "        \"model\": \"Llama-3.1-8B-Instruct\",\n",
    "        \"dataset\": \"bangla-empathic\",\n",
    "        \"task\": \"instruction-finetuning\",\n",
    "        \"language\": \"bangla\",\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": 2,\n",
    "        \"gradient_accumulation_steps\": 8,\n",
    "        \"effective_batch_size\": 16,\n",
    "        \"learning_rate\": 5e-5,  # Fixed: Reduced from 2e-4 to 5e-5 for better convergence\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"max_seq_length\": 2020,\n",
    "    },\n",
    "    tags=[\"llama-3.1\", \"bangla\", \"empathic\", \"unsloth\", \"lora\", \"fixed-training\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T12:05:34.127095Z",
     "iopub.status.busy": "2025-11-29T12:05:34.126740Z",
     "iopub.status.idle": "2025-11-29T12:05:34.139316Z",
     "shell.execute_reply": "2025-11-29T12:05:34.138737Z",
     "shell.execute_reply.started": "2025-11-29T12:05:34.127071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DatasetProcessor:\n",
    "    # Process and format datasets for Llama 3.1 fine-tuning\n",
    "    \n",
    "    def __init__(self, csv_path):\n",
    "        self.csv_path = csv_path\n",
    "        self.df = None\n",
    "        self.train_df = None\n",
    "        self.val_df = None\n",
    "        self.test_df = None\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "    \n",
    "    def load_and_clean_data(self):\n",
    "        # Load and clean the dataset\n",
    "        self.df = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Keep only Questions and Answers columns\n",
    "        self.df = self.df[['Questions', 'Answers']].copy()\n",
    "        \n",
    "        # Remove rows with missing values\n",
    "        self.df = self.df.dropna()\n",
    "        \n",
    "        # Remove rows where text is empty after stripping whitespace\n",
    "        self.df = self.df[(self.df['Questions'].str.strip() != '') & (self.df['Answers'].str.strip() != '')]\n",
    "        \n",
    "        # Strip whitespace\n",
    "        self.df['Questions'] = self.df['Questions'].str.strip()\n",
    "        self.df['Answers'] = self.df['Answers'].str.strip()\n",
    "        \n",
    "        print(f\"Dataset size after cleaning: {len(self.df)}\")\n",
    "        return self.df\n",
    "    \n",
    "    def split_data(self, test_size=0.2, val_size=0.5, random_state=42):\n",
    "        # Split data into train, validation, and test sets\n",
    "        # First split: 80% train, 20% temp (for val + test)\n",
    "        self.train_df, temp_df = train_test_split(\n",
    "            self.df, test_size=test_size, random_state=random_state, shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Second split: Split temp into 50% validation, 50% test (10% each of total)\n",
    "        self.val_df, self.test_df = train_test_split(\n",
    "            temp_df, test_size=val_size, random_state=random_state, shuffle=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Training samples: {len(self.train_df)}\")\n",
    "        print(f\"Validation samples: {len(self.val_df):>6}\")\n",
    "        print(f\"Test samples: {len(self.test_df):>6}\")\n",
    "        \n",
    "        return self.train_df, self.val_df, self.test_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_prompt(question, answer=None):\n",
    "        # Format prompt using Llama 3.1 official format\n",
    "        # Reference: https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/\n",
    "        # Fixed: Added proper newlines before <|eot_id|> tokens\n",
    "        prompt = (\n",
    "            \"<|begin_of_text|>\"\n",
    "            \"<|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "            \"You are a sympathetic and helpful assistant. You answer people's questions in Bengali language.\\n<|eot_id|>\"\n",
    "            \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            f\"{question}\\n<|eot_id|>\"\n",
    "            \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        )\n",
    "        if answer:\n",
    "            prompt += f\"{answer}\\n<|eot_id|>\"\n",
    "        return prompt\n",
    "    \n",
    "    def formatting_prompts_func(self, examples):\n",
    "        # Format dataset examples for training\n",
    "        questions = examples['Questions']\n",
    "        answers = examples['Answers']\n",
    "        texts = [self.format_prompt(q, a) for q, a in zip(questions, answers)]\n",
    "        return {\"text\": texts}\n",
    "    \n",
    "    def create_datasets(self):\n",
    "        # Create HuggingFace datasets from dataframes\n",
    "        self.train_dataset = Dataset.from_pandas(self.train_df[['Questions', 'Answers']].reset_index(drop=True))\n",
    "        self.val_dataset = Dataset.from_pandas(self.val_df[['Questions', 'Answers']].reset_index(drop=True))\n",
    "        self.test_dataset = Dataset.from_pandas(self.test_df[['Questions', 'Answers']].reset_index(drop=True))\n",
    "        \n",
    "        # Apply formatting\n",
    "        self.train_dataset = self.train_dataset.map(self.formatting_prompts_func, batched=True)\n",
    "        self.val_dataset = self.val_dataset.map(self.formatting_prompts_func, batched=True)\n",
    "        self.test_dataset = self.test_dataset.map(self.formatting_prompts_func, batched=True)\n",
    "        \n",
    "        return self.train_dataset, self.val_dataset, self.test_dataset\n",
    "    \n",
    "    def display_sample(self, num_samples=2):\n",
    "        # Display sample formatted data\n",
    "        print(\"Sample formatted training data:\")\n",
    "        print(\"=\"*80)\n",
    "        for i in range(min(num_samples, len(self.train_dataset))):\n",
    "            print(f\"\\nSample {i+1}:\")\n",
    "            print(\"=\"*80)\n",
    "            print(self.train_dataset[i][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T12:05:34.141243Z",
     "iopub.status.busy": "2025-11-29T12:05:34.140865Z",
     "iopub.status.idle": "2025-11-29T12:05:34.166804Z",
     "shell.execute_reply": "2025-11-29T12:05:34.166282Z",
     "shell.execute_reply.started": "2025-11-29T12:05:34.141225Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LLAMAFineTuner:\n",
    "    # Fine-tune Llama 3.1 model with LoRA\n",
    "    \n",
    "    def __init__(self, model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct\", \n",
    "                 max_seq_length=2000, dtype=None, load_in_4bit=True, device_map=\"auto\"):\n",
    "        self.model_name = model_name\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.dtype = dtype\n",
    "        self.load_in_4bit = load_in_4bit\n",
    "        self.device_map = device_map\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.trainer = None\n",
    "    \n",
    "    def load_model(self):\n",
    "        # Load the base model and tokenizer\n",
    "        # Fixed: Enable 4bit loading by default to avoid meta tensor issues\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=self.model_name,\n",
    "            max_seq_length=self.max_seq_length,\n",
    "            dtype=self.dtype,\n",
    "            load_in_4bit=self.load_in_4bit,\n",
    "            device_map=self.device_map,\n",
    "            # Fix: Add these parameters to prevent meta tensor issues\n",
    "            trust_remote_code=True,\n",
    "            use_cache=False,\n",
    "        )\n",
    "        print(f\"Model loaded: {self.model_name}\")\n",
    "        return self.model, self.tokenizer\n",
    "    \n",
    "    def apply_lora(self, r=16, lora_alpha=16, lora_dropout=0, bias=\"none\",\n",
    "                   use_gradient_checkpointing=\"unsloth\", random_state=3407):\n",
    "        # Apply LoRA configuration to model\n",
    "        self.model = FastLanguageModel.get_peft_model(\n",
    "            self.model,\n",
    "            r=r, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                          \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout, # Supports any, but = 0 is optimized\n",
    "            bias=bias, # Supports any, but = \"none\" is optimized\n",
    "            # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "            use_gradient_checkpointing=use_gradient_checkpointing, # True or \"unsloth\" for very long context\n",
    "            random_state=random_state,\n",
    "            use_rslora=False, # We support rank stabilized LoRA\n",
    "            loftq_config=None, # And LoftQ\n",
    "        )\n",
    "        print(\"LoRA configuration applied\")\n",
    "        return self.model\n",
    "    \n",
    "    def create_trainer(self, train_dataset, val_dataset, per_device_train_batch_size=2, \n",
    "                       gradient_accumulation_steps=8, num_train_epochs=3, learning_rate=2e-4, \n",
    "                       output_dir=\"outputs\"):\n",
    "        # Create SFT trainer with optimal batch configuration\n",
    "        # Effective Batch Size = per_device_train_batch_size * gradient_accumulation_steps\n",
    "        # Default: 2 * 8 = 16 (recommended for stable training)\n",
    "        \n",
    "        effective_batch_size = per_device_train_batch_size * gradient_accumulation_steps\n",
    "        print(f\"Batch configuration:\")\n",
    "        print(f\"  Per-device batch size: {per_device_train_batch_size}\")\n",
    "        print(f\"  Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "        print(f\"  Effective batch size: {effective_batch_size}\")\n",
    "        \n",
    "        self.trainer = SFTTrainer(\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset, # Use validation set for monitoring\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=self.max_seq_length,\n",
    "            dataset_num_proc=2,\n",
    "            packing=False, # Can make training 5x faster for short sequences\n",
    "            \n",
    "            args=TrainingArguments(\n",
    "                per_device_train_batch_size=per_device_train_batch_size,\n",
    "                per_device_eval_batch_size=2,\n",
    "                gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "                warmup_steps=100, # Increased warmup for better stability\n",
    "                \n",
    "                # Choose one: num_train_epochs OR max_steps\n",
    "                num_train_epochs=num_train_epochs, # For full training - trains through entire dataset\n",
    "                # max_steps=max_steps, # Or use this for quick testing\n",
    "                learning_rate=learning_rate,\n",
    "                fp16=not is_bfloat16_supported(),\n",
    "                bf16=is_bfloat16_supported(),\n",
    "                logging_steps=10,\n",
    "                optim=\"adamw_8bit\",\n",
    "                weight_decay=0.01,\n",
    "                lr_scheduler_type=\"cosine\",\n",
    "                seed=42,\n",
    "                output_dir=output_dir,\n",
    "                \n",
    "                # Evaluation and checkpointing\n",
    "                eval_strategy=\"steps\",\n",
    "                eval_steps=50,\n",
    "                save_strategy=\"steps\",\n",
    "                save_steps=100,\n",
    "                save_total_limit=3,\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"eval_loss\",\n",
    "                greater_is_better=False,\n",
    "                \n",
    "                # Weights & Biases integration\n",
    "                report_to=\"wandb\",\n",
    "                run_name=\"llama-3.1-8b-finetuning-v2\",\n",
    "                logging_first_step=True,\n",
    "                logging_nan_inf_filter=True,\n",
    "                remove_unused_columns=False,\n",
    "                \n",
    "                # Fix: Add these to prevent meta tensor issues\n",
    "                dataloader_pin_memory=False,\n",
    "                gradient_checkpointing=True,\n",
    "                # Fix: Use safer data loading options\n",
    "                dataloader_num_workers=0,\n",
    "            ),\n",
    "        )\n",
    "        print(\"Trainer configured\")\n",
    "        return self.trainer\n",
    "    \n",
    "    def train(self):\n",
    "        # Train the model using unsloth's optimized training method\n",
    "        # Fixed: Use unsloth_train instead of standard trainer.train() to avoid meta tensor issues\n",
    "        print(\"Starting training...\")\n",
    "        from unsloth import unsloth_train\n",
    "        trainer_stats = unsloth_train(\n",
    "            self.trainer,\n",
    "            resume_from_checkpoint=None,\n",
    "            # Add safety parameters\n",
    "            max_steps=None,  # Let epochs control training duration\n",
    "        )\n",
    "        print(\"Training completed\")\n",
    "        return trainer_stats\n",
    "    \n",
    "    def save_model(self, output_dir=\"llama-3.1-8b-bangla-empathic-lora\"):\n",
    "        # Save the fine-tuned model\n",
    "        self.model.save_pretrained(output_dir)\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        # Optionally save to merged 16bit\n",
    "        try:\n",
    "            self.model.save_pretrained_merged(\"llama-3.1-8b-bangla-empathic-merged\", self.tokenizer, save_method=\"merged_16bit\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save merged model: {e}\")\n",
    "            print(\"LoRA adapter saved successfully though.\")\n",
    "        print(f\"Model saved to: {output_dir}\")\n",
    "    \n",
    "    def enable_inference_mode(self):\n",
    "        # Enable inference mode for the model\n",
    "        FastLanguageModel.for_inference(self.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T12:05:34.167967Z",
     "iopub.status.busy": "2025-11-29T12:05:34.167597Z",
     "iopub.status.idle": "2025-11-29T12:05:34.194673Z",
     "shell.execute_reply": "2025-11-29T12:05:34.194221Z",
     "shell.execute_reply.started": "2025-11-29T12:05:34.167941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    # Evaluate fine-tuned model\n",
    "    \n",
    "    def __init__(self, fine_tuner, data_processor):\n",
    "        self.model = fine_tuner.model\n",
    "        self.tokenizer = fine_tuner.tokenizer\n",
    "        self.trainer = fine_tuner.trainer\n",
    "        self.test_df = data_processor.test_df\n",
    "        self.test_dataset = data_processor.test_dataset\n",
    "        self.format_prompt = data_processor.format_prompt\n",
    "    \n",
    "    def generate_response(self, question, max_new_tokens=256, temperature=0.7, top_p=0.9):\n",
    "        # Generate a response for a question\n",
    "        prompt = self.format_prompt(question)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            use_cache=True\n",
    "        )\n",
    "        \n",
    "        # Decode and extract only the assistant's response\n",
    "        full_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Extract response after the prompt\n",
    "        response = full_output[len(self.tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)):].strip()\n",
    "        return response\n",
    "    \n",
    "    def evaluate_metrics(self, num_samples=100):\n",
    "        # Evaluate model with BLEU, ROUGE, and Perplexity\n",
    "        import numpy as np\n",
    "        from evaluate import load\n",
    "        \n",
    "        # Load evaluation metrics \n",
    "        bleu_metric = load(\"bleu\")\n",
    "        rouge_metric = load(\"rouge\")\n",
    "        \n",
    "        # Generate predictions on test set\n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        for i in range(min(num_samples, len(self.test_dataset))):\n",
    "            question = self.test_df.iloc[i]['Questions']\n",
    "            reference = self.test_df.iloc[i]['Answers']\n",
    "            prediction = self.generate_response(question)\n",
    "            predictions.append(prediction)\n",
    "            references.append(reference)\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        bleu_results = bleu_metric.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        rouge_results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "        \n",
    "        # Calculate perplexity from evaluation loss\n",
    "        eval_results = self.trainer.evaluate(eval_dataset=self.test_dataset)\n",
    "        perplexity = np.exp(eval_results['eval_loss'])\n",
    "        \n",
    "        results = {\n",
    "            \"perplexity\": perplexity,\n",
    "            \"bleu\": bleu_results['bleu'],\n",
    "            \"rouge1\": rouge_results['rouge1'],\n",
    "            \"rouge2\": rouge_results['rouge2'],\n",
    "            \"rougeL\": rouge_results['rougeL']\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nEvaluation Results:\")\n",
    "        print(f\"Perplexity: {results['perplexity']:.4f}\")\n",
    "        print(f\"BLEU Score: {results['bleu']:.4f}\")\n",
    "        print(f\"ROUGE-1: {results['rouge1']:.4f}\")\n",
    "        print(f\"ROUGE-2: {results['rouge2']:.4f}\")\n",
    "        print(f\"ROUGE-L: {results['rougeL']:.4f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def create_human_eval_samples(self, sample_size=20, output_file=\"human_evaluation_samples.csv\"):\n",
    "        # Create samples for human evaluation\n",
    "        import random\n",
    "        \n",
    "        # Sample random examples for human evaluation\n",
    "        sample_indices = random.sample(range(len(self.test_df)), min(sample_size, len(self.test_df)))\n",
    "        human_eval_data = []\n",
    "        \n",
    "        for idx in sample_indices:\n",
    "            question = self.test_df.iloc[idx]['Questions']\n",
    "            reference = self.test_df.iloc[idx]['Answers']\n",
    "            \n",
    "            # Get model prediction\n",
    "            prediction = self.generate_response(question)\n",
    "            \n",
    "            human_eval_data.append({\n",
    "                \"id\": idx,\n",
    "                \"question\": question,\n",
    "                \"reference_answer\": reference,\n",
    "                \"model_answer\": prediction,\n",
    "                \"empathy_score\": None, # To be filled by human evaluators (1-5)\n",
    "                \"relevance_score\": None, # To be filled by human evaluators (1-5)\n",
    "                \"fluency_score\": None, # To be filled by human evaluators (1-5)\n",
    "                \"notes\": \"\" # Additional comments\n",
    "            })\n",
    "        \n",
    "        # Save to CSV for human evaluation\n",
    "        human_eval_df = pd.DataFrame(human_eval_data)\n",
    "        human_eval_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"\\nCreated {len(human_eval_data)} samples for human evaluation\")\n",
    "        print(f\"Saved to: {output_file}\")\n",
    "        print(\"\\nEvaluation criteria:\")\n",
    "        print(\"1. Empathy Score (1-5): How empathetic and understanding is the response?\")\n",
    "        print(\"2. Relevance Score (1-5): How relevant is the response to the question?\")\n",
    "        print(\"3. Fluency Score (1-5): How fluent and natural is the Bengali language?\")\n",
    "        \n",
    "        return human_eval_df\n",
    "    \n",
    "    def display_sample_responses(self, num_samples=5):\n",
    "        # Display sample responses with streaming\n",
    "        from transformers import TextStreamer\n",
    "        print(f\"Generating {num_samples} sample responses on test prompts\")\n",
    "       \n",
    "        \n",
    "        for i in range(min(num_samples, len(self.test_df))):\n",
    "            question = self.test_df.iloc[i]['Questions']\n",
    "            reference = self.test_df.iloc[i]['Answers']\n",
    "            \n",
    "            print(f\"\\n--- Sample {i+1} ---\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"\\nReference Answer: {reference}\")\n",
    "            print(f\"\\nModel Response:\")\n",
    "            \n",
    "            # Create prompt\n",
    "            prompt = self.format_prompt(question)\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            \n",
    "            # Generate with streaming\n",
    "            text_streamer = TextStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "            _ = self.model.generate(\n",
    "                **inputs,\n",
    "                streamer=text_streamer,\n",
    "                max_new_tokens=300,\n",
    "                temperature=0.5,\n",
    "                top_p=0.9,\n",
    "                use_cache=True\n",
    "            )\n",
    "            \n",
    "            print(\"\\n\" + \"-\"*80)\n",
    "    \n",
    "    def log_all_responses(self, experiment_name=\"llama-3.1-8b-bangla-empathic\"):\n",
    "        # Log all test responses\n",
    "        from datetime import datetime\n",
    "        import uuid\n",
    "        \n",
    "        # Generate experiment ID\n",
    "        experiment_id = f\"{experiment_name}-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "        \n",
    "        # Generate responses for all test samples and log them\n",
    "        generated_responses = []\n",
    "        \n",
    "        print(f\"Generating and logging responses for {len(self.test_df)} test samples...\")\n",
    "        \n",
    "        for idx in range(len(self.test_df)):\n",
    "            question = self.test_df.iloc[idx]['Questions']\n",
    "            reference = self.test_df.iloc[idx]['Answers']\n",
    "            response_text = self.generate_response(question)\n",
    "            \n",
    "            # Create log entry\n",
    "            log_entry = {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"experiment_id\": experiment_id,\n",
    "                \"sample_index\": idx,\n",
    "                \"input_text\": question,\n",
    "                \"reference_text\": reference,\n",
    "                \"response_text\": response_text,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"model_name\": \"llama-3.1-8b-instruct\",\n",
    "                \"temperature\": 0.5,\n",
    "                \"top_p\": 0.9,\n",
    "                \"max_new_tokens\": 300\n",
    "            }\n",
    "            \n",
    "            generated_responses.append(log_entry)\n",
    "            \n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"Processed {idx + 1}/{len(self.test_df)} samples...\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        responses_df = pd.DataFrame(generated_responses)\n",
    "        log_filename = f\"generated_responses_{experiment_id}.csv\"\n",
    "        responses_df.to_csv(log_filename, index=False)\n",
    "        \n",
    "        print(f\"\\nGenerated responses logged successfully!\")\n",
    "        print(f\"Total responses: {len(generated_responses)}\")\n",
    "        print(f\"Saved to: {log_filename}\")\n",
    "        \n",
    "        return responses_df, log_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "eb9412e3-a0ae-4091-bb13-9de72737823a",
    "_uuid": "3b8a32a3-b074-4767-867f-411a4a343ff6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T12:05:34.195854Z",
     "iopub.status.busy": "2025-11-29T12:05:34.195511Z",
     "iopub.status.idle": "2025-11-29T12:05:34.218037Z",
     "shell.execute_reply": "2025-11-29T12:05:34.217290Z",
     "shell.execute_reply.started": "2025-11-29T12:05:34.195828Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 15.8 + 15.8 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(1)}\")\n",
    "    print(\n",
    "        f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} + {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "d0da5b53-af13-4ce5-80b2-78d6cd56251d",
    "_uuid": "eb274f00-f6e9-4d7f-a013-228cbf3b23b1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T12:05:34.219871Z",
     "iopub.status.busy": "2025-11-29T12:05:34.219177Z",
     "iopub.status.idle": "2025-11-29T12:05:34.874280Z",
     "shell.execute_reply": "2025-11-29T12:05:34.873670Z",
     "shell.execute_reply.started": "2025-11-29T12:05:34.219853Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after cleaning: 38210\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>আমার স্ত্রী এবং মায়ের মধ্যে টানটান মতবিরোধ চল...</td>\n",
       "      <td>আপনি যা বর্ণনা করছেন তাকে মনোবিজ্ঞানীরা \"ত্রিভ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>আমি বাচ্চা নেওয়ার পরিকল্পনা করছি, তাই আমাকে ধ...</td>\n",
       "      <td>হাই। আপনার শিশুর (এবং নিজের) জন্য যা স্বাস্থ্য...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>আমার মনের মধ্যে গোপন আছে, এবং আমি জানি না তাদে...</td>\n",
       "      <td>মনে হচ্ছে গোপন রাখা এখন আপনার জন্য একটি সমস্যা...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>আমি আমার সম্পর্কের ক্ষেত্রে অত্যন্ত অধিকারসূচক...</td>\n",
       "      <td>হ্যালো। এটা দুর্দান্ত যে আপনি উপলব্ধি করতে সক্...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>কয়েক বছর আগে আমার মাথায় আঘাত লেগেছিল এবং আমা...</td>\n",
       "      <td>আপনি বলেননি কি বা কত ওষুধ আপনি চেষ্টা করেছেন। ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Questions  \\\n",
       "0  আমার স্ত্রী এবং মায়ের মধ্যে টানটান মতবিরোধ চল...   \n",
       "1  আমি বাচ্চা নেওয়ার পরিকল্পনা করছি, তাই আমাকে ধ...   \n",
       "2  আমার মনের মধ্যে গোপন আছে, এবং আমি জানি না তাদে...   \n",
       "3  আমি আমার সম্পর্কের ক্ষেত্রে অত্যন্ত অধিকারসূচক...   \n",
       "4  কয়েক বছর আগে আমার মাথায় আঘাত লেগেছিল এবং আমা...   \n",
       "\n",
       "                                             Answers  \n",
       "0  আপনি যা বর্ণনা করছেন তাকে মনোবিজ্ঞানীরা \"ত্রিভ...  \n",
       "1  হাই। আপনার শিশুর (এবং নিজের) জন্য যা স্বাস্থ্য...  \n",
       "2  মনে হচ্ছে গোপন রাখা এখন আপনার জন্য একটি সমস্যা...  \n",
       "3  হ্যালো। এটা দুর্দান্ত যে আপনি উপলব্ধি করতে সক্...  \n",
       "4  আপনি বলেননি কি বা কত ওষুধ আপনি চেষ্টা করেছেন। ...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize DatasetProcessor\n",
    "data_processor = DatasetProcessor('/kaggle/input/bengali-empathetic-conversations-corpus/BengaliEmpatheticConversationsCorpus .csv')\n",
    "\n",
    "# Load and clean data\n",
    "df = data_processor.load_and_clean_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "626a8440-adcc-4b5c-9d08-630b7675839b",
    "_uuid": "b00230af-b065-4d04-aa16-104b78ca85d8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T12:05:34.875140Z",
     "iopub.status.busy": "2025-11-29T12:05:34.874959Z",
     "iopub.status.idle": "2025-11-29T12:06:49.075239Z",
     "shell.execute_reply": "2025-11-29T12:06:49.074427Z",
     "shell.execute_reply.started": "2025-11-29T12:05:34.875126Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03345cc424714d688fed11931871e2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[accelerate.big_modeling|WARNING]Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: unsloth/Meta-Llama-3.1-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLAMAFineTuner\n",
    "# Fixed: Enable load_in_4bit=True to prevent meta tensor issues\n",
    "fine_tuner = LLAMAFineTuner(\n",
    "    model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length=2020, # Choose any! We auto support RoPE Scaling internally!\n",
    "    dtype=None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit=True, # Fixed: Enable 4-bit quantization to prevent meta tensor issues\n",
    "    device_map=\"auto\" # Auto device allocation\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = fine_tuner.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_cell_guid": "d1eb6ba2-0f8e-4aeb-9eaa-0162b75ab440",
    "_uuid": "960fd236-32a8-4507-ad68-ee2d6520f0ef",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T12:06:49.077548Z",
     "iopub.status.busy": "2025-11-29T12:06:49.077272Z",
     "iopub.status.idle": "2025-11-29T12:06:49.099927Z",
     "shell.execute_reply": "2025-11-29T12:06:49.099201Z",
     "shell.execute_reply.started": "2025-11-29T12:06:49.077530Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 30568\n",
      "Validation samples:   3821\n",
      "Test samples:   3821\n"
     ]
    }
   ],
   "source": [
    "# Train/Validation/Test Splits\n",
    "train_df, val_df, test_df = data_processor.split_data(test_size=0.2, val_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_cell_guid": "4ff7045c-791b-426b-b291-87c30bc7de80",
    "_uuid": "1d0ca9d9-6ab8-46a8-9440-1a9e52210a53",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T12:06:49.100968Z",
     "iopub.status.busy": "2025-11-29T12:06:49.100651Z",
     "iopub.status.idle": "2025-11-29T12:06:56.554861Z",
     "shell.execute_reply": "2025-11-29T12:06:56.553958Z",
     "shell.execute_reply.started": "2025-11-29T12:06:49.100945Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147794036b2c425790cab0a4ecba8c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30568 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5be15df308448d861aec8f0ac16810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2008933a914ec78f7c2b946cc20122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 4: CREATE HUGGINGFACE DATASETS\n",
    "train_dataset, val_dataset, test_dataset = data_processor.create_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T12:06:56.556431Z",
     "iopub.status.busy": "2025-11-29T12:06:56.556200Z",
     "iopub.status.idle": "2025-11-29T12:06:56.562986Z",
     "shell.execute_reply": "2025-11-29T12:06:56.562244Z",
     "shell.execute_reply.started": "2025-11-29T12:06:56.556414Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample formatted training data:\n",
      "================================================================================\n",
      "\n",
      "Sample 1:\n",
      "================================================================================\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a sympathetic and helpful assistant. You answer people's questions in Bengali language.\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "আমি আগামী ফেব্রুয়ারিতে প্রমোশন পাওয়ার চেষ্টা করছি।\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "আরে। আমি আপনি এটা পেতে আশা করি!\n",
      "<|eot_id|>\n",
      "\n",
      "Sample 2:\n",
      "================================================================================\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a sympathetic and helpful assistant. You answer people's questions in Bengali language.\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "আমি নিজেকে রক্ষা করেছি\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "এটা চমৎকার, আমি আশা করি আপনি আঘাত পাবেন না!\n",
      "<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Display sample data\n",
    "data_processor.display_sample(num_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_cell_guid": "5278f42b-88f6-41d8-ab13-6b5de7c292aa",
    "_uuid": "7fc8fa98-3b0f-4126-9601-f1747d95e75e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T12:06:56.563901Z",
     "iopub.status.busy": "2025-11-29T12:06:56.563649Z",
     "iopub.status.idle": "2025-11-29T12:07:05.154427Z",
     "shell.execute_reply": "2025-11-29T12:07:05.153189Z",
     "shell.execute_reply.started": "2025-11-29T12:06:56.563878Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA configuration applied\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA\n",
    "model = fine_tuner.apply_lora(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=2020\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_cell_guid": "b22387d9-853c-4a3b-8bf3-d8696c98021b",
    "_uuid": "625bdee0-6722-4ae1-9883-913bccb6b12a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T12:07:54.859938Z",
     "iopub.status.busy": "2025-11-29T12:07:54.859319Z",
     "iopub.status.idle": "2025-11-29T12:07:54.958751Z",
     "shell.execute_reply": "2025-11-29T12:07:54.957813Z",
     "shell.execute_reply.started": "2025-11-29T12:07:54.859909Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch configuration:\n",
      "  Per-device batch size: 2\n",
      "  Gradient accumulation steps: 8\n",
      "  Effective batch size: 16\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47/1676234817.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# - Learning rate reduced to 5e-5 for better convergence past plateaus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m trainer = fine_tuner.create_trainer(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_47/2767866694.py\u001b[0m in \u001b[0;36mcreate_trainer\u001b[0;34m(self, train_dataset, val_dataset, per_device_train_batch_size, gradient_accumulation_steps, num_train_epochs, learning_rate, output_dir)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Effective batch size: {effective_batch_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         self.trainer = SFTTrainer(\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/trainer.py\u001b[0m in \u001b[0;36mnew_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0moriginal_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/working/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func, **kwargs)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0munsloth_zoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_utils\u001b[0m  \u001b[0;32mimport\u001b[0m \u001b[0mfix_zero_training_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'tokenizer'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessing_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         \u001b[0mfix_untrained_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIGNORED_TOKENIZER_NAMES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m         \u001b[0mfix_zero_training_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/tokenizer_utils.py\u001b[0m in \u001b[0;36mfix_untrained_tokens\u001b[0;34m(model, tokenizer, train_dataset, IGNORED_TOKENIZER_NAMES, eps)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0mlm_head_where\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindicator_untrained1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mlm_head_bad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm_head_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlm_head_where\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_head_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mlm_head_bad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm_head_bad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data!"
     ]
    }
   ],
   "source": [
    "# Create trainer with optimal batch configuration and full epoch training\n",
    "# Fixed: Changed from max_steps=100 to num_train_epochs=3\n",
    "# - max_steps=100 was only 13% of one epoch (causing loss plateau at 0.6)\n",
    "# - num_train_epochs=3 will train through entire dataset 3 times (~2,250 steps)\n",
    "# - Learning rate reduced to 5e-5 for better convergence past plateaus\n",
    "\n",
    "trainer = fine_tuner.create_trainer(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    per_device_train_batch_size=2,  # Primary driver of VRAM usage\n",
    "    gradient_accumulation_steps=8,  # Primary driver of training time\n",
    "    num_train_epochs=3,  # Train for 3 full epochs instead of stopping at 100 steps\n",
    "    learning_rate=5e-5,  # Reduced from 2e-4 to help break through loss plateaus\n",
    "    output_dir=\"outputs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_cell_guid": "14ff21a4-bc89-4e07-9b5d-a64449219000",
    "_uuid": "ae5c4cd4-8a7f-4ede-bc10-27d5add88609",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-29T12:07:35.822286Z",
     "iopub.status.busy": "2025-11-29T12:07:35.822000Z",
     "iopub.status.idle": "2025-11-29T12:07:35.860840Z",
     "shell.execute_reply": "2025-11-29T12:07:35.859915Z",
     "shell.execute_reply.started": "2025-11-29T12:07:35.822266Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47/58937333.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfine_tuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_47/2767866694.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# Note: Using trainer.train() instead of unsloth_train to avoid meta tensor issues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mtrainer_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training completed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer_stats = fine_tuner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bf6f273a-dd64-4886-98e1-28f1f6056c09",
    "_uuid": "b3f5e14f-4adc-4c03-a051-e098032b11f5",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-11-29T12:07:05.419805Z",
     "iopub.status.idle": "2025-11-29T12:07:05.420114Z",
     "shell.execute_reply": "2025-11-29T12:07:05.420011Z",
     "shell.execute_reply.started": "2025-11-29T12:07:05.420000Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "fine_tuner.save_model(\"llama-3.1-8b-bangla-empathic-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0b7c67b3-6a4e-4cf5-b071-82c3541f238b",
    "_uuid": "4a6f28e0-d3bd-4736-90bc-7a574245844b",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-11-29T12:07:05.421977Z",
     "iopub.status.idle": "2025-11-29T12:07:05.422225Z",
     "shell.execute_reply": "2025-11-29T12:07:05.422123Z",
     "shell.execute_reply.started": "2025-11-29T12:07:05.422113Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Enable inference mode and initialize Evaluator\n",
    "fine_tuner.enable_inference_mode()\n",
    "evaluator = Evaluator(fine_tuner, data_processor)\n",
    "\n",
    "# Evaluate using: Perplexity, BLEU, ROUGE\n",
    "results = evaluator.evaluate_metrics(num_samples=10)\n",
    "\n",
    "# Log to wandb\n",
    "wandb.log(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "df259160-896f-44cd-b081-65ce46bee118",
    "_uuid": "3ab1c13a-f6ee-41f6-a292-fc8d483e5956",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-11-29T12:07:05.423041Z",
     "iopub.status.idle": "2025-11-29T12:07:05.423283Z",
     "shell.execute_reply": "2025-11-29T12:07:05.423176Z",
     "shell.execute_reply.started": "2025-11-29T12:07:05.423166Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate: Human evaluation on empathetic response quality\n",
    "human_eval_df = evaluator.create_human_eval_samples(sample_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b160cb3a-68c6-4cda-90dd-531d6c958d4b",
    "_uuid": "6398a7d1-4c40-458f-980f-b2a6ce76399e",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-11-29T12:07:05.424389Z",
     "iopub.status.idle": "2025-11-29T12:07:05.424697Z",
     "shell.execute_reply": "2025-11-29T12:07:05.424556Z",
     "shell.execute_reply.started": "2025-11-29T12:07:05.424541Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Sample model responses on test prompts\n",
    "evaluator.display_sample_responses(num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1fa3e950-b4ff-460b-8000-f92145bf0241",
    "_uuid": "a5ca8ca0-87e8-43ac-b2b4-6634f7569ca2",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-11-29T12:07:05.425706Z",
     "iopub.status.idle": "2025-11-29T12:07:05.426033Z",
     "shell.execute_reply": "2025-11-29T12:07:05.425889Z",
     "shell.execute_reply.started": "2025-11-29T12:07:05.425875Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Store logs for GeneratedResponses: id, experiment_id, input_text, response_text, timestamp\n",
    "responses_df, log_filename = evaluator.log_all_responses()\n",
    "\n",
    "# Upload to wandb as artifact\n",
    "artifact = wandb.Artifact(\n",
    "    name=\"generated-responses\",\n",
    "    type=\"predictions\",\n",
    "    description=\"Generated responses from fine-tuned model on test set\"\n",
    ")\n",
    "artifact.add_file(log_filename)\n",
    "wandb.log_artifact(artifact)\n",
    "print(\"Logged to W&B as artifact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ba37c577-a9e7-4065-89f7-f95b5b9c9f5e",
    "_uuid": "a8b421f5-afe7-4ffb-be3b-4f6eb3cfd321",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-11-29T12:07:05.427182Z",
     "iopub.status.idle": "2025-11-29T12:07:05.427484Z",
     "shell.execute_reply": "2025-11-29T12:07:05.427318Z",
     "shell.execute_reply.started": "2025-11-29T12:07:05.427305Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Finish wandb run\n",
    "wandb.finish()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training and evaluation completed successfully!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel saved to: llama-3.1-8b-bangla-empathic-lora\")\n",
    "print(f\"Human evaluation samples: human_evaluation_samples.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3497143,
     "sourceId": 6104837,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
